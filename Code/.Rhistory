renv::restore()
install.packages("renv")
install.packages(c("boot", "class", "codetools", "foreign", "KernSmooth", "lattice", "MASS", "Matrix", "mgcv", "nlme", "nnet", "spatial", "survival"), lib="/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library")
renv::rebuild()
library(tidyverse)
library(gt)
library(GPfit)
library(BASS)
install.packages("BASS")
updateR()
install.packages(packageurl, repos=NULL, type="source")
packageurl <- "http://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.9.1.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
packageurl <- "https://cran.r-project.org/src/contrib/Archive/BASS/BASS_1.2.0.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
packageurl <- "https://cran.r-project.org/src/contrib/Archive/BASS/BASS_1.2.0.tar.gz"
install.packages(packageurl, repos=NULL, type="source", dependencies = TRUE)
install.packages("truncdist", "gsl")
packageurl <- "https://cran.r-project.org/src/contrib/Archive/BASS/BASS_1.2.0.tar.gz"
install.packages(packageurl, type="source", dependencies = TRUE)
install.packages("truncdist")
install.packages("gsl")
packageurl <- "https://cran.r-project.org/src/contrib/Archive/BASS/BASS_1.2.0.tar.gz"
install.packages(packageurl, type="source", dependencies = TRUE)
library(tidyverse)
library(gt)
library(GPfit)
library(BASS)
library(lhs)
library(plotly)
library(MASS)
f <- function(x1,x2) {
y1 <- x1 * 2 + 0.5
y2 <- x2 * 2 + 0.5
return((sin(10 * pi * y1)/(2 * y2) + (y1-1)^4))
}
f <- function(x1,x2) {
y1 <- x1 * 15
y2 <- x2 * 20 - 5
a <- 1
b <- 5.1 / (4 * pi ^2)
c <- 5 / pi
r <- 6
s <- 10
t <- 1 / (8 * pi)
return(a * (y2 - b * y1^2 + c * y1 - r)^2 + s * (1-t) * cos(y1) + s)
}
n0 = 10
XX <- maximinLHS(n0, 2)
Y <- f(XX[,1], XX[,2])
fit <- bass(XX,Y, verbose = FALSE, nmcmc = 100000, nburn = 90000)
plot(fit)
plot(fit)
get_bass_2d <- function(max_iterations = 100){
n0 <- 10
dims <- 2
XX <- maximinLHS(n0, dims)
Y <- f(XX[,1], XX[,2])
for (iteration in 1:max_iterations) {
fit <- bass(
xx = XX,
y = Y,
degree = 1,
verbose = F,
#thin = 10
nmcmc=1000,
nburn=9000
)
x_new <- maximinLHS(1000,dims)
pred <- predict(fit, newdata = data.frame(x = x_new))
mu <- apply(pred,2,mean)
kappa <- 90 # tunable
sigma <- apply(pred,2,sd)
lower_confidence_bound <- mu - kappa * sigma
XX <- rbind(XX, x_new[which.min(lower_confidence_bound),])
Y <- append(Y, f(x_new[which.min(lower_confidence_bound),][1],x_new[which.min(lower_confidence_bound),][2]))
print(iteration)
}
return(Y)
}
one_run <- get_bass_2d(max_iterations = 1)
one_run <- get_bass_2d(max_iterations = 1)
one_run <- get_bass_2d(max_iterations = 1)
get_bass_2d <- function(max_iterations = 100){
n0 <- 10
dims <- 2
XX <- maximinLHS(n0, dims)
Y <- f(XX[,1], XX[,2])
for (iteration in 1:max_iterations) {
fit <- bass(
xx = XX,
y = Y,
degree = 1,
verbose = F,
#thin = 10
nmcmc=1000,
nburn=900
)
x_new <- maximinLHS(1000,dims)
pred <- predict(fit, newdata = data.frame(x = x_new))
mu <- apply(pred,2,mean)
kappa <- 90 # tunable
sigma <- apply(pred,2,sd)
lower_confidence_bound <- mu - kappa * sigma
XX <- rbind(XX, x_new[which.min(lower_confidence_bound),])
Y <- append(Y, f(x_new[which.min(lower_confidence_bound),][1],x_new[which.min(lower_confidence_bound),][2]))
print(iteration)
}
return(Y)
}
one_run <- get_bass_2d(max_iterations = 1)
plot(one_run)
get_bass_2d <- function(max_iterations = 100){
n0 <- 10
dims <- 2
XX <- maximinLHS(n0, dims)
Y <- f(XX[,1], XX[,2])
for (iteration in 1:max_iterations) {
fit <- bass(
xx = XX,
y = Y,
degree = 1,
verbose = F,
#thin = 10
nmcmc=1000,
nburn=900
)
x_new <- maximinLHS(1000,dims)
pred <- predict(fit, newdata = data.frame(x = x_new))
mu <- apply(pred,2,mean)
kappa <- 9 # tunable
sigma <- apply(pred,2,sd)
lower_confidence_bound <- mu - kappa * sigma
XX <- rbind(XX, x_new[which.min(lower_confidence_bound),])
Y <- append(Y, f(x_new[which.min(lower_confidence_bound),][1],x_new[which.min(lower_confidence_bound),][2]))
print(iteration)
}
return(Y)
}
one_run <- get_bass_2d(max_iterations = 1)
plot(one_run)
```{r packages}
library(tidyverse)
library(gt)
library(GPfit)
library(BASS)
library(lhs)
library(plotly)
library(MASS)
library(tidyverse)
library(gt)
library(GPfit)
library(BASS)
library(lhs)
library(plotly)
library(MASS)
library(MASS)  # For the Boston housing dataset
# Load the necessary packages
library(glmnet)
install.packages("glmnet")
# Load the necessary packages
library(glmnet)
library(MASS)  # For the Boston housing dataset
# Load the Boston housing dataset
data(Boston)
housing <- as.data.frame(Boston)
# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(housing), 0.7 * nrow(housing))
train_data <- housing[train_indices, ]
test_data <- housing[-train_indices, ]
# Preprocess the data (scale and center)
train_data_scaled <- scale(train_data[, -c(14)])
test_data_scaled <- scale(test_data[, -c(14)])
# Create matrices for the predictors and response variables
x_train <- as.matrix(train_data_scaled[, -c(14)])
y_train <- train_data_scaled[, 14]
View(Boston)
Boston
View(Boston)
View(train_data_scaled)
train_data$indus
hist(train_data$indus)
?hist
hist(train_data$indus, breaks = "50")
hist(train_data$indus, breaks = 50)
# Load the Boston housing dataset
data(Boston)
housing <- as.data.frame(Boston)
# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(housing), 0.7 * nrow(housing))
train_data <- housing[train_indices, ]
test_data <- housing[-train_indices, ]
# Preprocess the data (scale and center)
train_data_scaled <- scale(train_data[, -c(14)])
test_data_scaled <- scale(test_data[, -c(14)])
# Create matrices for the predictors and response variables
x_train <- as.matrix(train_data_scaled[, -c(14)])
y_train <- train_data_scaled[, 14]
# Load the Boston housing dataset
housing <- as.data.frame(Boston)
# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(housing), 0.7 * nrow(housing))
train_data <- housing[train_indices, ]
test_data <- housing[-train_indices, ]
# Preprocess the data (scale and center)
train_data_scaled <- scale(train_data[, -14])  # Exclude the response variable (column 14)
test_data_scaled <- scale(test_data[, -14])  # Exclude the response variable (column 14)
# Create matrices for the predictors and response variables
x_train <- as.matrix(train_data_scaled[, -14])  # Exclude the response variable (column 14)
y_train <- train_data_scaled[, 14]  # Select only the response variable (column 14)
train_data_scaled
View(train_data_scaled)
View(Boston)
# Load the Boston housing dataset
housing <- as.data.frame(Boston)
# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(housing), 0.7 * nrow(housing))
train_data <- housing[train_indices, ]
test_data <- housing[-train_indices, ]
# Preprocess the data (scale and center)
train_data_scaled <- scale(train_data[, -14])  # Exclude the response variable (column 14)
test_data_scaled <- scale(test_data[, -14])  # Exclude the response variable (column 14)
# Create matrices for the predictors and response variables
x_train <- as.matrix(train_data_scaled[, -14])  # Exclude the response variable (column 14)
y_train <- train_data[, 14]  # Select only the response variable (column 14)
x_test <- as.matrix(test_data_scaled[, -14])  # Exclude the response variable (column 14)
y_test <- test_data[, 14]  # Select only the response variable (column 14)
# Perform elastic net regression
enet_model <- glmnet(x_train, y_train, alpha = 0.5)  # alpha = 0.5 for balanced elastic net
# Predict on the test set
enet_predictions <- predict(enet_model, newx = x_test)
# Calculate the root mean squared error (RMSE) on the test set
rmse <- sqrt(mean((enet_predictions - y_test)^2))
cat("RMSE:", rmse)
?glmnet
plot(enet_model)
?BASS
?bass
?bass
y<-rnorm(n,f(x),sigma)
## simulate data (Friedman function)
f<-function(x){
10*sin(pi*x[,1]*x[,2])+20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma<-1 # noise sd
n<-500 # number of observations
x<-matrix(runif(n*10),n,10) #10 variables, only first 5 matter
y<-rnorm(n,f(x),sigma)
## simulate data (Friedman function)
f<-function(x){
10*sin(pi*x[,1]*x[,2])+20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma<-1 # noise sd
n<-500 # number of observations
x<-matrix(runif(n*10),n,10) #10 variables, only first 5 matter
y<-rnorm(n,f(x),sigma)
## simulate data (Friedman function)
f<-function(x){
10*sin(pi*x[,1]*x[,2])+20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma<-1 # noise sd
n<-500 # number of observations
x<-matrix(runif(n*10),n,10) #10 variables, only first 5 matter
y<-rnorm(n,f(x),sigma)
10*sin(pi*x[,1]*x[,2])+20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
## simulate data (Friedman function)
f<-function(x){
10*sin(pi*x[,1]*x[,2])+20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma<-1 # noise sd
n<-500 # number of observations
x<-matrix(runif(n*10),n,10) #10 variables, only first 5 matter
y<-rnorm(n,f(x),sigma)
## simulate data (Friedman function)
f<-function(x){
10*sin(pi*x[,1]*x[,2])+20*(x[,3]-.5)^2+10*x[,4]+5*x[,5]
}
sigma<-1 # noise sd
n<-500 # number of observations
x<-matrix(runif(n*10),n,10) #10 variables, only first 5 matter
y<-rnorm(n,f(x),sigma)
```{r}

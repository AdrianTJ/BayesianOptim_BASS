---
title: "https://github.com/bearloga/bayesopt-tutorial-r/blob/master/index.Rmd"
output: html_notebook
---

```{r packages}
library(tidyverse)
library(gt)
library(GPfit)
library(BASS)
```

```{r function, echo=TRUE}
f <- function(x) {
  y <- x * 2 + 0.5
  return((sin(10 * pi * y)/(2 * y) + (y-1)^4))
}
```

```{r function2, echo=TRUE}
f <- function(x) {
  return(sin(2 * pi * (x - 0.1)))
}
```

```{r function3, echo=TRUE}
f <- function(x) {
  s <- x * 10
  return(sin(2 * pi * s / 10 ) + sin(2 * pi * s / 2.5))
}
```

```{r evaluations}
# seed with a few evaluations:
n0 <- 100
evaluations <- matrix(
  as.numeric(NA),
  ncol = 2, nrow = n0,
  dimnames = list(NULL, c("x", "y"))
)
evaluations[, "x"] <- seq(0, 1, length.out = n0)
evaluations[, "y"] <- f(evaluations[, "x"]) 
evaluations %>%
  as_tibble
```

# Iterative run

```{r}
max_iterations = 30

# seed with a few evaluations:
n0 <- 5
evaluations <- matrix(
  as.numeric(NA),
  ncol = 2, nrow = n0,
  dimnames = list(NULL, c("x", "y"))
)
evaluations[, "x"] <- seq(0, 1, length.out = n0)
evaluations[, "y"] <- f(evaluations[, "x"]) 

for (iteration in 1:max_iterations) {

fit <- bass(
  xx = evaluations[, "x"],
  y = evaluations[, "y"],
  degree = 1,
  verbose = F,
  #nmcmc = 60000,
  #nburn=30000,
  #thin = 10
)


x_new <- seq(0, 1, length.out = 1000)
pred <- predict(fit, newdata = data.frame(x = x_new))
mu <- pred[1000,]
X <- evaluations[, "x"]

mesh <- seq(0,1, length.out = 1000)

y  <- NULL;
for (elem in 1:length(X)){
  tmp <- abs(mesh - X[elem])
  y <- rbind(y, tmp)
}
finals <- y |> apply(2,min)
sigma <- (apply(pred, 2, sd)/max(apply(pred, 2, sd)))


kappa <- 9 # tunable
lower_confidence_bound <- mu - kappa * sigma


evaluations <- rbind(evaluations,c(x_new[which.min(lower_confidence_bound)] , f(x_new[which.min(lower_confidence_bound)])))
}
```

```{r}
evaluations
```

# Plot

```{r, fig.width=8, fig.height=4}
plot_posterior <- function() {
  plot(
    x_new, mu,
    type = "l", col = "blue", lwd = 2, lty = "dotted",
    ylim = c(-2, 5),
    xlab = "x", ylab = "f(x)", main = "Posterior of f"
  )
  polygon(
    c(x_new, rev(x_new)),
    c(mu + sigma, rev(mu - sigma)),
    col = rgb(0, 0, 1, 0.25), border = NA
  )
  points(evaluations, pch = 16)
  legend(
    "topleft",
    c(expression(f[n[0]]), expression(mu(x)), expression(mu(x) %+-% sigma(x)), expression(f)),
    col = c("black", "blue", "blue", "red"), pch = c(16, NA, NA, NA),
    lty = c(NA, "dotted", NA, NA), lwd = c(NA, 2, 1,NA), bty = "n",
    fill = c(NA, NA, rgb(0, 0, 1, 0.25), rgb(1, 0, 0, 1)),
    border = c(NA, NA, NA, NA), ncol = 4, text.width = 0.1
  )
  lines(seq(0, 1, length.out = 1000), f(seq(0, 1, length.out = 1000)), col = 'red')
}
par(cex = 1.1, mfrow = c(1, 1), mar = c(5.1, 4.1, 4.1, 2.1))
plot_posterior()
```


```{r, layout="l-body-outset", fig.width=10, fig.height=5}
par(cex = 1.1, mfrow = c(1, 2))
plot(
  x_new, lower_confidence_bound,
  type = "l", col = "red",
  xlab = "x", ylab = expression("a"["LCB"]),
  main = "GP lower confidence bound"
)
plot_posterior()
```



```{r}
plot(fit)
```

```{r}
summary(fit)
```





















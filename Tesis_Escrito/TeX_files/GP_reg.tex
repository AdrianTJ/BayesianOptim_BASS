\chapter{Gaussian Process Regression}

One of the applications of Gaussian processes is to use them as a method to perform supervised learning using it. The method generates from data a mean function and then uses that mean function to extrapolate or predict on points where there is no data observed. As we described in the last chapter, we can fully specify or determine a particular Gaussian process by specifying its mean $m(x) = \mathbb{E}[f(x)]$ and its kernel function $k(x,x')$. While in the last chapter we started with the Gaussian process and from that generate the values on the rest of the input space, in this chapter we are going to be doing somewhat of the inverse of that. We will be starting from a set of data $\{ x_i \}_{i \leq n} = \mathcal{D} \subset \mathcal{X}$, and adjusting the properties of the kernel and mean to achieve the best fit Gaussian process to the points and minimize the error. 

One of the advantages of Gaussian process regression over other forms of supervised learning algorithms (such as linear regression) is the fact that Gaussian processes are a non-parametric model. Without delving too much into the specifics, this means that the number of parameters controlling the model is not fixed at a certain quantity, but rather is adaptive. The amount of parameters can grow and shrink depending on the data that is being fit. 

This however does not mean that we do not have control over the behavior of the function we decide to adjust to the points. In the following figure, we can see how changing the hyperparameter $\ell$ in the squared exponential covariance function changes the relative smoothness of the fit function. 

\begin{figure}[h]
	\includegraphics[width=4cm]{Figures/missing.png}
	\centering
	\caption{Gaussian Processes for ML, page 20.}
	\label{trajectories}
\end{figure}

We also note how radically the confidence intervals change from one adjustment to the next based on moving this hyperparameter around. Because we can define an infinite amount of Gaussian processes based on our choice of hyperparameters (and further even by changing the kernel function), \textcolor{red}{HOW TO STATE THIS AS A MINIMIZATION PROBLEM?}
This problem translates to finding under the given assumptions the function that best adjusts to these points in the sense that it minimizes the uncertainty present. This will involve the selection of the best\footnote{Best is a dangerous word in this context. Obviously depending on the problem and the minimization objectives, this best set of hyperparameters that define this function can change, but here we simply refer to the one or family of functions that solves the above minimization problem.} hyperparameters given the constraints of the problem and using that defined mean function to predict values not in the data set.

To get there, we will begin by going over regression in general, then make the link to the formal theory of Gaussian process regression to show how supervised learning can be performed by this method. 

\subsection{Preamble to Gaussian Process Regression: A Rehash on Regression Analysis}

The central task of this section is regression, which is the process of estimating or generating understanding of a theoretical function $f: \mathcal{X} \rightarrow \mathbb{R}$ that describes the relationship between a response variable $y$ and a group or single input variable described by the vector $\boldsymbol{x}$. While we do not know the exact behavior of $f$ the function that describes this relationship\footnote{We will discuss this in later chapters, but this is actually one of the greatest advantages of Bayesian Optimization, in that in can work on so called \textit{black-box} functions}, what we do have is a set of observations $\mathcal{D} \subset \mathcal{X}$, where $\mathcal{D} = \{ (x_i, y_i), i = 1,2,\ldots, n \}$ and $x_i \in \mathbb{R}^d$ and $y_i \in \mathbb{R}$. These observations constitute point-wise realizations of the function $f$. From that, for a set of new unknown points $\boldsymbol{x}_*$ we want to predict or infer the quantity $f(x_{i*}) = y_*$, which would be point-wise estimates of these unseen or unknown realizations of $f$. 

One example of another method od solving this problem is the well known and well loved linear regression, that assumes that the function $f$ is of the general form 
\[ y = f(x) = x^T \beta + \epsilon \]
where $\epsilon \sim \mathcal{N}(0, \sigma^2)$. The error variance $\sigma^2$ and the coefficients $\beta$ are generated by a process of minimizing the error of the outputs generated. 

One of the disadvantages of these types of methods is that they only give one function as the output, but there may well be an infinite number of functions that fit a certain set of points. As an example, consider the following figure. 

\begin{figure}[h]
	\includegraphics[width=4cm]{Figures/missing.png}
	\centering
	\caption{Gaussian Processes for ML, page 20.}
	\label{trajectories}
\end{figure}

Notice how for the training dataset $\mathcal{D}$, all of these functions have error zero! The only recourse we have is to restrict these functions to generate one function (or at least a finite family of functions), and not an infinity of them. As with many other areas of mathematics, there is a trade off that we have to choose to perform between how general a solution to a problem is and how many solutions to that problem we wish to get. \textcolor{red}{Expand on this.} 

The key insight to understanding Gaussian process regression from a statistician's point of view is changing the way we think about regression. Instead of going the traditional way and providing restrictions to generate particular approximations, why don't we consider the space of possible functions that can fit these points as coming from a probability model? What if the observed functions in the past figure are realizations of a more general class of random variable that is coherent in the function space, rather than the discrete variable or vector space? From this groundwork is where Gaussian processes come into the picture. The only added restriction that we impose on the functions is that as a family, they come from a generalization to infinite dimensions of the multivariate normal distribution, and with that, we generate the entire conceptual framework from which we derive and generate the Gaussian process regresion method. Having now set the expectation of what we wish to accomplish, we can now construct the theory of this different class of regression analysis. 

\subsection{Theory of Gaussian Process Regression (from the function space view)}

This section of the text is heavily based on the work and structure of \cite{rasmussen2006gaussian} and to a much lesser extent \cite{wang2020intuitive}. The book Gaussian Processes for Machine Learning is the go-to reference and source for modern applications of Gaussian processes in general and it would be a disservice to the reader to not follow that structure and argumentation. 

We can split up the set $\mathcal{D}$ into training and test, defining $\mathcal{D}_{train} = \{ x_i , y_i \}_{i=1}^n$, and $\mathcal{D}_{test} = \{ x^*_i , y^*_i \}_{i=1}^{^*}$, with $x_i, x^*_i \in \mathbb{R}^d$ and $y_i, y^*_i \in \mathbb{R}$. As described before, what we want to achieve is basically generating out of sample predictions for the test set $D_{test}$. Since the function $f = m$ defined is the mean of the process we defined before, we can simply evaluate the out of sample points $x_i^*$ on $f$ and we have an estimate for $y_i^*$. For simplicity and by convention, we denote the set of of test outputs $\boldsymbol{f}_*$. Because of the structure we have defined, formally, the regression function $f$ is really a normal distribution when conditioned by the training dataset $\mathcal{D}_{train}$, which would be
\[ \mathbb{P}(\boldsymbol{f} | \boldsymbol{X}) = \mathcal{N}(f | \mu, k) \]
where $\boldsymbol{X} = [ x_1, x_2, \ldots , x_n ]$, $\boldsymbol{f} = [ f(x_1), f(x_2), \ldots, f(x_n) ]$, $\mu= [ m(x_1), m(x_2), \ldots, m(x_n) ]$ and $k$ the kernel or covariance function of the process. By convention, it is customary to set $m(\boldsymbol{X}) = 0$, but there is a reasoning behind this. When dealing with the type of data generally modelled by these types of functions, it is common (encouraged even) to normalize the data before applying these methods to it. Another important piece of shorthand notation we will use is $K=k(\boldsymbol{X}, \boldsymbol{X})$, $K_*=k(\boldsymbol{X}, \boldsymbol{X}_*)$, and $K_{**}=k(\boldsymbol{X}_*, \boldsymbol{X}_*)$. Note that since the number of training points need not equal the number of test points, then $n$ has not attributable relationship with $n^*$ and we cannot state any properties of the matrix $K_*$, much less say that it is a square matrix. 

Since we have formally defined the distribution of $\boldsymbol{f}$ and we know that $\boldsymbol{f}_*$ is the same distribution but considering a different set of data points $\boldsymbol{X}_* = [x^*_1, \ldots, x^*_n]$, we can define the joint distribution of these vectors as 
\[ \begin{bmatrix}
	\boldsymbol{f} \\
	\boldsymbol{f}_*
\end{bmatrix} \sim \mathcal{N}\left( \begin{bmatrix}
0 \\
0
\end{bmatrix} , \begin{bmatrix}
K & K_* \\
K_*^T & K_{**}
\end{bmatrix} \right), 
\]

which is the joint probability distribution $\mathbb{P}(\boldsymbol{f}, \boldsymbol{f}_* | \boldsymbol{X}, \boldsymbol{X}_*)$. This is a massive step in constructing the regression model but we are still missing one component. When dealing with regression and prediction of values outside of the original training dataset, we do not need the joint probability, but the conditional probability of the test set given everything else we know. Formally, this means that we need to find the probability distribution of $\mathbb{P}(\boldsymbol{f}_* | \boldsymbol{X}, \boldsymbol{X}_*, \boldsymbol{f})$. Luckily, since we are dealing with Gaussian distributions, there is a very neat closed form expression that describes this probability. Referring to section $A.2$ of \cite{rasmussen2006gaussian}, and noting that the mean for both $\boldsymbol{f}$ and $\boldsymbol{f}_*$ is 0, then 
\[ \boldsymbol{f}_* | \boldsymbol{f} \sim \mathcal{N}(K_*^T K^{-1} \boldsymbol{f}, K_{**} - K_*^T K^{-1} K_*). \]

This probability function is what is used to calculate out of sample predictions for the out of sample data $\boldsymbol{X}_*$. 
 


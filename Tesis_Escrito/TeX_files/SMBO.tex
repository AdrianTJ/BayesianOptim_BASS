\chapter{Sequential Model Based Optimization}

There has been a lot of work done, primarily over the last few years, in the space of Bayesian Optimization. One primary advance has been the formalization of sequential model based optimization (SMBO) as a general framework for Bayesian Optimization. The general idea of these types of models is to divide the optimization problem into two stages, first fitting a model from a specific class that predicts the performance on the target function and then gathering additional data to further refine the model that is used as a surrogate for the actual function we wish to evaluate. As detailed in \cite{conceptualBO}, the two steps can be broken down into this pseudo-algorithm for the problem of finding the best hyperparameters for a generic machine learning model: 

\begin{enumerate}
	\item Build a surrogate probability model for the target function
	\item Find the hyperparameters that perform the best on the surrogate
	\item Apply these hyperparameters to the true objective function\footnote{This is the part that is expensive to evaluate, ideally, we want to minimize the amount of times that we actually run the true objective function.}
	\item Update the surrogate model incorporating the new results
	\item Repeat steps 2-4 until a limit is reached (maximum number of iterations, time spent, or others such as cost).
\end{enumerate}

While all SMBO algorithms follow these basic steps, the true power of it is in the flexibility it provides when selecting the surrogate model and when selecting how the new results can be incorporated into the model itself (steps 3 and 4). \textcolor{red}{expand on this, it is the main point of the thesis.} 

As is clear by the title of this work, the main contribution is the implementation of the BASS model as a surrogate model under this framework, but there have been a large amount of different methods and models used as surrogate models as well as a large amount of ways of incorporating the results into the surrogate, known as the selection function or evaluation criteria depending on the literature being read. The next sections detail the other models that are commonly used as surrogate models, and where these models got their roots.

\section{A Survey of Existing Surrogate Models}

Different surrogate models can capture different aspects of the underlying function, each with its own strengths and weaknesses. The most common choice is Gaussian Processes (GPs). The selection depends on factors such as the dimensionality of the problem, the smoothness of the objective function, its domain (fully continuous, hybrid or discrete), and the available computational resources. In this section, we provide some texture as to the common surrogate models for the framework. 

\subsection{Gaussian Processes}
While much time has already been dedicated to GPs in this work, it is worth noting that the strengths that define this method merit it. First and foremost, the models provide a robust probabilistic framework under which to model the underlying function. They not only predict the mean of the function but also provide a measure of uncertainty baked in (variance and covariance) associated with the predictions outside of the already estimated points. This uncertainty estimation is what allows Bayesian Optimization to work, since the selection of the next best point to evaluate is generally the point that we have the most uncertainty about, coupled with it being a proper candidate for a minima or maxima, depending on the situation. The choice between selecting candidates based on uncertainty or chances of being a better choice than the already explored points give us a trade-off in that we can tune depending on what we find most relevant for the problem by adjusting the models hyperparameters. 

While these models do not work well for combinatorial domains\footnote{This is a very loaded statement, but the authors think it is correct. There is also literature to back this up, such as \cite{wang2023recent} \S 3.2.} because these models explicitly assume a continuous search space, if the problem has a relatively low dimension (not uncommon for hyperparameter optimization), then it is very efficient computationally to calculate. There is also the matter of kernel selection, which gives the user unparalleled flexibility in continuous domains, since the kernel can be adapted to predict in a family of functions (such as strictly linear ones) if there is information known about the behaviour of the objective function already known. 

Another advantage that cannot be overstated is the interpretability of these Gaussian Processes. These models not only provide mean value calculations, but also confidence intervals. This makes it easier to explain why the model is making certain choices, and with most types of problems, it is generally preferable to start with a simple and understandable method and scale up if necessary than to use a highly complex method which might become its own black box. Because of the nature of these models, they also tend to work well with small amounts of data, which goes hand in hand nicely with the restriction that we set off with for Bayesian Optimization, that being that the objective function is expensive to evaluate. This becomes a problem when dealing with surrogate models such as neural networks, which require orders of magnitude more data to make proper predictions. 

Yet another large benefit worth discussing is the fact that these models interpolate. The black box objective functions that we are assigning uncertainties to and calculating pointwise values for are at the completely deterministic. What the framework of Bayesian Optimization allows us to do is assign that uncertainty based on our own standing relative to the function, this is, the functions themselves are not probabilistic or uncertain, but our understanding of them is. Other surrogate models such as BASS which we are proposing do not interpolate perfectly, in the case of BASS because the estimation is done through MCMC which generates a probability distribution over a point in this use case. Because of their underlying construction and properties Gaussian Processes not only interpolate with zero variance in the points already calculated, but assign a low variance to regions of the domain close to the ones that have been calculated, and as such reduce the need for unnecessary exploration which is important under the framework that the objective functions are costly to evaluate.

To conclude, it is important to note that while all of these theoretical benefits are incredibly important, it is also worth mentioning that there is a lot of work in existing libraries and frameworks that has already been done to facilitate the use of GPs in the context of Bayesian Optimization. Because it is the de-facto standard in the area, most all of the existing packages for SMBO and BO specifically include modules and tools to define a Gaussian Process as the surrogate model. Practical inertia must not be discounted as a critical reason to explain the prevalence of GPs in this field. 

\subsection{Tree Parzen Estimators}

The inception of this algorithm comes from the paper \cite{bergstra2011algorithms}, which provides a thorough explanation of the mechanisms that define TPEs (Tree Parzen Estimators). Even though this algorithm was created in 2013, it is still considered one of the premier algorithms for hyperparameter tuning using BO. This algorithm excels at problems with high dimensionality and small fitness evaluation budgets. The success of TPEs lies in their ability to efficiently explore the hyperparameter space by modelling the objective function using a combination of tree-based estimators and probabilistic density estimation techniques, such as Parzen estimators (also referred to as kernel density estimators), which entail a straightforward averaging of kernels centered around existing data points. Probably the most interesting aspect of this surrogate model is that while we normally are interested in calculating $\mathbb{P}(y | x)$ which is the probability of an objective function given the points already calculated, TPE takes a different approach and calculates $\mathbb{P}(x | y)$, which is the probability of having the points already calculated given a certain objective function being the \say{correct} one. 

By iteratively updating a probabilistic model of the objective function and using it to guide the search for promising hyperparameter configurations, TPEs strike a balance between exploration and exploitation, allowing it to converge to near-optimal solutions with relatively few evaluations. One large benefit to TPE models is that there is only one hyperparameter to tune, namely $\gamma \in [0,1]$, which controls what we consider to be a good function evaluation and what we consider to be a bad one. Going into mode detail than this would require a section onto itself, but more details can be found in \cite{bergstra2011algorithms} and in the paper for Hyperopt, which is the python library published by the creators of the model for hyperparameter tuning in python \cite{bergstra2013hyperopt}. One important thing to consider is that these types of models require more data to achieve great results when compared to GPs in general, and as such, if the function is prohibitively expensive, it could become a problem. 

\subsection{Bayesian Neural Networks}

Bayesian neural networks (BNNs) are a type of neural network that incorporates Bayesian methods to model uncertainty in the network's weights and predictions. Traditional neural networks typically output a point estimate for each input, representing a deterministic prediction. In contrast, BNNs provide a probabilistic framework that quantifies uncertainty associated with predictions. Bayesian neural networks, with their probabilistic framework, provide a natural way to model the objective function and its uncertainty. During the optimization process, BNNs can be trained on the observed data points (input-output pairs), and the resulting posterior distribution over the weights can be used to make predictions about the objective function's behaviour in unobserved regions of the input space. 

Furthermore, BNNs offer the advantage of not only providing point estimates of the objective function but also quantifying the uncertainty associated with those estimates. This uncertainty estimation is crucial in BO for guiding the exploration-exploitation trade-off. 

This area of research is thriving, and many different neural network architectures have been proposed to further optimize the BO model. This work does not delve into any particular methodology, but a run-down of recent advances and their particular use cases (e.g., large scale data, high dimensional BO, combinatorial optimization) can be seen in \cite{wang2023recent}.

\subsection{Random Forest, Gradient Boosted Machines}

Tree and forest based algorithms have received a lot of praise in recent years. They are unreasonably effective when dealing with tabular data in a traditional setting, even outperforming deep neural networks which seem to have overtaken these models in everything but tabular data \cite{grinsztajn2022tree}. In the context of BO, they can be good surrogate models, as they are generally good regressors. They are highly flexible, as they can automatically handle nonlinearities, interactions, and high-dimensional feature spaces without the need for explicit feature engineering. They are also highly scalable, as they can handle a wide range of problem sizes and complexities, making them suitable for BO tasks with varying levels of computational resources.

While these models may seem like a natural choice, there are some limitations in selecting them as surrogate models, most relevantly, the lack of probabilistic outputs. Unlike Bayesian Neural Networks (BNNs) or GPs, RFs and GBMs do not naturally provide probabilistic outputs or uncertainty estimates. This makes it more challenging to quantify uncertainty in predictions, which is crucial for guiding the exploration-exploitation trade-off in BO. This is lessened by the fact that they can indirectly capture uncertainty through techniques such as bootstrapping and out-of-bag estimation, and as such, these methods can be leveraged to estimate the variability in predictions and guide the exploration-exploitation trade-off in BO. There are also some other limitations, like the fact that these models are basically black boxes in and of themselves if the problem is of a high dimension or there are many interactions between variables, or that if we are not careful with regularization, they can be overfit very easily. One other large problem is simply the amount of hyperparameters these models have, as tuning them to perform well on a BO problem is not an easy task, and changes between one objective function and another can make previous hyperparameter configurations unusable, or lead to model stagnation, which is not something we want with objective functions that are expensive to evaluate. 

A practical advantage that cannot be overlooked is the sheer amount of libraries and support that exists for these types of models. They have been studied intensely for the last decades, and the library implementations that exist in popular programming languages are some of the best designed Machine Learning libraries out there. Still, the choice of a RF or GBM must be done carefully, as heavy trade-offs could hurt more than help in the long run. 

\section{Other Techniques for Optimizing Hyperparameters}

\subsection{Grid Search}

In grid search, the hyperparameter space is divided into a grid, and models are evaluated at each point in the grid. While grid search is exhaustive and guaranteed to find the optimal solution within the search space. The largest trade-off is that this method of search is expensive, since each function evaluation is defined by the problem to be expensive. If this is not a problem, then this is one of the best methods out there. The constraint of expensive to evaluate black-box functions is what necessitates the rise of other search algorithms, such as Bayesian Optimization. 

\subsection{Random Search}

This is a simple technique where hyperparameters are randomly selected from a predefined search space. Despite its simplicity, random search can often be surprisingly effective and computationally efficient. One of the main problems comes when the search space becomes complex, because of the curse of dimensionality, using a random search strategy could lead to a very narrow search in terms of the space as a whole. This can be compounded by not looking at points \say{close} to the ones where we already found good results, which can lead to us not finding local or global maximums or minimums, and instead spending valuable time looking at regions we might already know to probably not yield interesting results. A good reference for both grid search methods and random search can be found in \cite{bergstra2012random}.

\subsection{Genetic Algorithms}

Part of the larger group composed of evolutionary algorithms, they are based on the idea of natural selection when evaluating certain points in the hyperparameter mesh that we define. To extend the analogy, the fittest (in this case the hyperparameter vectors with the lowest or highest values) are selected to product offspring. This is done iteratively, and these individuals traits (good guesses for the optimums) are passed on to the next generations. These algorithms are generally more complex than anything else covered in this work and as such we do not elaborate much further, but a good and recent reference is \cite{xiao2020efficient}, which describes these algorithms used to tune hyperparameters in deep learning models. 
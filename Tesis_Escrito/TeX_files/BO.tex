\chapter{Bayesian Optimization}

\section{Introduction}

The necessity for Bayesian optimization comes from a relatively simple constraint problem, but not in the way we normally think of constraints when dealing with optimization. In traditional optimization problems, there are constraints to the structure of the function or the solutions that we can find. An example of this is integer optimization, where the objective function either only takes variables in a discrete form or we are interested only in the solutions to the problem that are integers. While we are fining local or global maxima or minina, what we generally want to solve when in the scope of traditional optimization is the number of steps we need to find an optimum element in a specific set to the function or set of functions we are evaluating. This remains true for Bayesian optimization, but there is an element that makes it unique- under this paradigm we consider the function that we are trying to find expensive to evaluate. What this means is that the nature and our approach to these types of problems has to change drastically, as we are not only concerned with finding the best solution to the problem, we are also interested in doing it in the most efficient way possible in terms of function evaluations. What this means generally is that we want to not only find the maxima or minima of the function, but we have this added restriction that we have to do it in the least number of steps possible. 

There are many scenarios where this paradigm is necessary because of the types of problems we are dealing with. One particular case which will become relevant to this work in later chapters is hyperparameter optimization in machine learning systems. When training large machine learning models as is becoming customary these days (look at the training time and size of for example \textcolor{red}{AGREGAR CITAS}), we want to do things in the most efficient manner possible. For many models, this means specifying things such as a learning rate in neural networks, which is the susceptibility of a certain model to change its weights depending on the error estimates generated. If this hyperparameter is set too high, and the model may be unstable in training and we may never reach a stable solution. If the weight is too low, then the model will change it's weights too slowly and we will be left with either too much time dedicated to training or simply does not achieve good metrics or results since there is still learning to be done by it o the data that is available. These hyperparameters are generally chosen through expert judgement, transferred over from other similar models, or randomly shuffled through until trying to explore the space in which these hyperparameters exist. Bayesian optimization is a different approach, in which a we assign probability of finding extrema in certain unexplored areas of the function space and iteratively search through this space until we either find these extrema or find a suitable candidate with a certain confidence that it is either the extrema or close to it. 

As the models that we build and the problems we solve in general become more and more complicated, there has been a growing interest in this area of statistics as a solution to generating viable extrema candidates of expensive to evaluate functions. There has been extensive work on this field recently and in the past few decades, with examples such as \textcolor{red}{Add examples, hyperopt, Botorch, etc.}

In this chapter we will show the basic theoretical structure of Bayesian optimization, the two central components that make it up, and the decisions we have to make to be able to approximate these functions from a Bayesian point of view. 

Bayesian optimization is a sequential model-based approach to optimizing black-box functions that are expensive to evaluate. The goal is to find the input $x$ that maximizes the objective function $f(x)$ with the minimum number of function evaluations.

The Bayesian optimization algorithm consists of three main components: a prior distribution over the objective function, an acquisition function, and a method for updating the prior distribution based on the observed data.

Let $f(x)$ be the objective function that we want to maximize, and let $D$ be the set of observations we have made so far, which consists of pairs $(x_i, y_i)$ where $x_i$ is the input and $y_i$ is the corresponding output of the function $f(x_i)$. We want to find the input $x$ that maximizes the function $f(x)$, i.e., $x^* = \arg\max_x f(x)$.

The prior distribution over the objective function $f(x)$ is usually assumed to be a Gaussian process (GP), which is a probabilistic model that defines a distribution over functions. Given a set of observations $D$, the GP defines a posterior distribution over functions that takes into account the uncertainty in the observations.

The acquisition function is a criterion that measures the utility of each point in the input space $x$ for the purpose of optimization. The most commonly used acquisition function is the expected improvement (EI), which measures the expected improvement in the objective function if we were to evaluate the function at a particular input $x$. The EI is defined as:

\[ EI(x) = \mathbb{E}[max(f(x) - f(x_{best}), 0)], \]

where $x_{\text{best}}$ is the current best input found so far, and $\mathbb{E}[\cdot]$ is the expectation operator. The idea is to select the input $x$ that maximizes the EI, which balances exploration (trying new inputs) and exploitation (focusing on inputs that are likely to be good).

The final component of the Bayesian optimization algorithm is the method for updating the prior distribution based on the observed data. This is done using Bayes' rule, which allows us to compute the posterior distribution over the objective function given the prior distribution and the observed data. The posterior distribution is then used as the prior distribution for the next iteration of the algorithm.

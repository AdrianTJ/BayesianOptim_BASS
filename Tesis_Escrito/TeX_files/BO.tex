\chapter{Bayesian Optimization}

This chapter (and the rest of this work, really) relies pretty heavily on the book \cite{garnett_bayesoptbook_2023} as a source of consistency and terminology. While at the time of writing this book is still in pre-print, this might be the most approachable and complete treatment of Bayesian optimization the author has found. We rely on it for standardizing notation and for some of the proofs and concepts, as well as for a general sketch of how this chapter is set out. 

\section{Introduction}

The necessity for Bayesian optimization comes from a relatively simple constraint problem, but not in the way we normally think of constraints when dealing with optimization. In traditional optimization problems, there are constraints to the structure of the function or the solutions that we can find. An example of this is integer optimization, where the objective function either only takes variables in a discrete form or we are interested only in the solutions to the problem that are integers. While we are fining local or global maxima or minina, what we generally want to solve when in the scope of traditional optimization is the number of steps we need to find an optimum element in a specific set to the function or set of functions we are evaluating. This remains true for Bayesian optimization, but there is an element that makes it unique- under this paradigm we consider the function that we are trying to find expensive to evaluate. What this means is that the nature and our approach to these types of problems has to change drastically, as we are not only concerned with finding the best solution to the problem, we are also interested in doing it in the most efficient way possible in terms of function evaluations. What this means generally is that we want to not only find the maxima or minima of the function, but we have this added restriction that we have to do it in the least number of steps possible. 

There are many scenarios where this paradigm is necessary because of the types of problems we are dealing with. One particular case which will become relevant to this work in later chapters is hyperparameter optimization in machine learning systems. When training large machine learning models as is becoming customary these days (look at the training time and size of for example \textcolor{red}{AGREGAR CITAS}), we want to do things in the most efficient manner possible. For many models, this means specifying things such as a learning rate in neural networks, which is the susceptibility of a certain model to change its weights depending on the error estimates generated. If this hyperparameter is set too high, and the model may be unstable in training and we may never reach a stable solution. If the weight is too low, then the model will change it's weights too slowly and we will be left with either too much time dedicated to training or simply does not achieve good metrics or results since there is still learning to be done by it o the data that is available. These hyperparameters are generally chosen through expert judgement, transferred over from other similar models, or randomly shuffled through until trying to explore the space in which these hyperparameters exist. Bayesian optimization is a different approach, in which a we assign probability of finding extrema in certain unexplored areas of the function space and iteratively search through this space until we either find these extrema or find a suitable candidate with a certain confidence that it is either the extrema or close to it. 

As the models that we build and the problems we solve in general become more and more complicated, there has been a growing interest in this area of statistics as a solution to generating viable extrema candidates of expensive to evaluate functions. There has been extensive work on this field recently and in the past few decades, with examples such as \textcolor{red}{Add examples, hyperopt, Botorch, etc.} Hyperparameter optimization has recently been talked about as the "killer app" for Bayesian optimization, which revitalized interest in Gaussian Processes in the context of machine learning and Bayesian optimization in general. The interest in the field has lead to a great amount of interest and therefore research and publications tackling the method and providing theoretical backing to it. 

In this chapter we will show the basic theoretical structure of Bayesian optimization, the three central components that make it up, and the decisions we have to make to be able to approximate these functions from a Bayesian point of view. 

\section{Components of BO}

Bayesian optimization is a sequential model-based approach to optimizing black-box functions that are expensive to evaluate. The goal is to find in some domain $\mathcal{X}$ a value $x$ that maximizes the objective function $f$ with the least possible number of evaluations. Practically, this means numerically calculating the value $f(x)$ for the least amount of times possible. One important distinction from other areas of optimization is that we do not impose any restrictions on the function $f$, except continuity\footnote{As we will see in later chapters, the entire point of this work will be relaxing this restriction.}. Formally, let $f$ be the objective function that we want to maximize, and let $\mathcal{D}$ be the set of observations we have made so far, which consists of pairs $(x_i, y_i)$ where $x_i$ is the input and $y_i$ is the corresponding output of the function $f(x_i)$. We want to find the input $x$ that maximizes the function $f(x)$, i.e., $x^* = \arg\max_\mathcal{X}  f(x)$.

The Bayesian optimization algorithm consists of three main components: a prior distribution over the objective function, an acquisition function, and a method for updating the prior distribution based on the observed data. The way this model works can be thought of as the following three steps: 
\begin{enumerate}
	\item Determining a probabilistic model that captures the observed behaviour of $f$, either by assuming a prior on the first step or going off data points $(x_i, f(x_i))$ for later iterations. We call this probabilistic mode of $f$ the prior, since it is used basically as one under this scheme\footnote{Another common way to jump-start this process is to begin by evaluating some random points in $\mathcal{X}$ and continuing the process from there. }.
	\item Optimizing the selected acquisition function that probabilistically estimates the best possible points for evaluation in the following iteration. 
	\item Evaluating the function at the best candidate for improvement, therefore getting new data and updating our probabilistic model of the behaviour of the function. 
\end{enumerate}

The prior distribution over the objective function $f(x)$ is usually assumed to be a Gaussian process, but that need not be the case. While these type of models are beyond the scope of this work, as examples, we refer to \cite{shah2013bayesian} for the prior set to a $t$-Student distribution and \cite{wangpoisson} for a case study of the Poisson prior. Given the set of observations $\mathcal{D}$, the selected prior distribution defines a posterior distribution that takes into account the uncertainty in the observations.

The acquisition function is a criterion that measures the expected utility of each point in the input space $x$ for the purpose of optimization. The most commonly used acquisition function is the expected improvement (EI), which measures the expected improvement in the objective function if we were to evaluate the function at a particular input $x$. Much like the case with the prior distributions, there are other choices such as Thompson sampling (TS) and Upper Confidence bound (UCF) \textcolor{red}{Add citations, in the Poisson process paper from last paragraph}. The EI is defined as:

\[ EI(x) = \mathbb{E}[max(f(x) - f(x_{best}), 0)], \]

where $x_{\text{best}}$ is the current best input found so far. The idea is to select the input $x$ that maximizes the EI, which balances exploration (trying new inputs) and exploitation (focusing on inputs that are likely to be good) this balance os controlled by hyperparameters, so that is another layer of complexity that needs to be controlled. In a sense, what we are doing is differing solving the main optimization problem by generating a simpler to solve optimization problem in finding the relevant extrema of the acquisition function, thus allowing us to spend as little function evaluations as possible on evaluating the target function $f$. 

The final component of BO is the method for updating the prior distribution based on the observed data. This is done under the overarching philosophy of Bayesian inference, which allows us to compute the posterior distribution over the objective function given the prior distribution and the observed data. The posterior distribution is then used as the prior distribution for the next iteration of the algorithm. What we wish to do is infer the value $\phi = f(x)$ for some unobserved point in the domain $\mathcal{X}$. 

\section{Formalization}

This process begins by setting the prior distribution $\mathbb{P}(\phi | x)$ which is our belief of possible values for $\phi$ before observation of the unknown $x$. 

Assuming we observe the objective function at $x$ and measure $y$, our optimization model assumes that the distribution of this measurement is determined by the value of interest $\phi$ through the observation model $\mathbb{P}(y | x, \phi)$. With this new value $y$ we can update our beliefs and generate the posterior function through the use of Bayes' theorem: 
\[ \mathbb{P}(\phi | x, y) = \frac{\mathbb{P}(\phi | x) \cdot \mathbb{P}(y | x, \phi)}{\mathbb{P}(y | x)}. \] 

\textcolor{red}{PAGES 7 and 8 of BO BOOK is where we are at.}
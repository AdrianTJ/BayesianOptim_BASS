\chapter{Stochastic Processes}

Before starting, we would like to note that sections of this chapter where constructed using different references as primary guides. The section on Markov chains primarily uses the references \cite{Zitkovic_2010} and \cite{hohendorff_introduction_2005}. 

\section{General Definitions}

Stochastic processes play a very important role in basically every part of this work. First, the basis of traditional Bayesian Optimization is Gaussian processes, which are a specific class of stochastic process in which all the points or vectors have a conjoined multi-normal distribution. This is not the only place where stochastic processes play a role, as BASS is a Bayesian method and modern Bayesian methods generally rely on simulation strategies such as Markov Chain Monte Carlo (MCMC) to numerically approximate complex posterior distributions. We use a particular class of MCMC algorithms called reverse jump MCMC when dealing with our alternate surrogate function, but that is detailed much later when we talk about the Bayesian Adaptive Regression Spline method. 

All in all, stochastic processes are generally important to the content of this work so we include some important results and definitions, starting with the definition of a stochastic process itself, taken from \cite[\S 3.0]{Zitkovic_2010}. 

\begin{definition}
	Let $\Theta \subseteq \mathbb{R}^+$. A \textit{stochastic process} $\{ X_\theta, \theta \in \Theta \}$ is a collection of random variables, indexed by a parameter $\theta$, such that $\theta$ belongs to some index set $\Theta$. When $\Theta = \mathbb{N}$ (or $\Theta = \mathbb{N}_0$), then it is called a \textit{discrete-time process}, and if $\Theta = \mathbb{R}^+$, then it is called a \textit{continuous-time process}. 
\end{definition}

To distinguish between a discrete and continuous time process, we use the index $n$ for discrete-time processes and $t$ for continuous-time processes. 
In all applications of this work the indexes refer to time, as the function spaces $\mathcal{X}$ on which the functions are evaluated are time dependent. 
This definition is very broad, as for example, if we take the case where $\Theta = \{1\}$, then the stochastic process $\{ X_\theta, \theta \in \Theta \}$ is really just $X_1$, a random variable. 
As with many other areas of mathematics though, the introduction of infinities in the values the indexes can take introduces a large amount of complexity, and things like vector distributions do no extend nicely to these new cases. 

% Duda
Another more formal definition of a stochastic process defines it as a function of the index and an element of the sample space, as follows: 

\begin{definition}
	Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space\footnote{Billingsley's classic book \cite{billingsley2012probability} has all the theoretical foundation of probability spaces and provides a thorough run down of these concepts.} where $\Omega$ is a sample space, $\mathcal{F}$ is a $\sigma$-algebra and $\mathbb{P}: \mathcal{F} \mapsto [0,1]$ a probability measure. Taking $\Theta$ an arbitrary index space, a stochastic process is a function 
	\[ X: \Omega \times \Theta \mapsto \mathbb{R} \]
	such that for all  $\theta \in \Theta$,
	\[ X_\theta : \omega \mapsto X(\omega, \theta) \equiv X_\theta : \Omega \mapsto \mathbb{R} \]
	is a measurable function, which is equivalent to saying that $X_\theta$ is a random variable.
\end{definition}

% Duda
When $\omega$ is known, $\theta \mapsto X(\omega, \theta): \Theta \mapsto \mathbb{R}$ is a sample function, also known as a realization of the stochastic process. These realizations are the trajectories or paths that are famous in diagrams such as the one shown in Figure \ref{trajectories}. An example application of this type of structure are time series, where the fundamental properties of stochastic processes are used to generate a function whose realizations move through time. 

\begin{figure}[h]
	\includegraphics[width=4cm]{Figures/missing.png}
	\centering
	\label{trajectories}
\end{figure}

As was mentioned earlier, the two main types of stochastic process that can be defined are ones where the realizations of the process occur in a discrete fashion, and the other is when they occur in a continuous fashion. It so happens that the two main structures relating to stochastic processes that we need to go into detail on are Markov chains, a discrete process, and Gaussian processes, which are continuous. We go in depth in the following sections into each of these types of stochastic processes, starting with Markov chains since Gaussian processes lead nicely onto the next chapter. 

\section{Markov Chains}

Markov chains are ubiquitous in their applications, since they represent a relatively simple but very powerful concept; the mathematical structure on which we can define phase transitions of position changes that are governed by probabilistic rules. The fundamental property which will be explored later is that independently of the past, the only important factor in determining the future state of the process is the current state of it. We begin with a formal definition and take a constructive approach in building this model. 

\begin{definition}
	Let $\{ X_n, n \in \mathbb{N}_0 \}$ be a stochastic process defined on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ such that
	\[ X_n: \omega \mapsto X(\omega, n) \equiv X_n : \Omega \mapsto \mathcal{S}, \]
	where $\mathcal{S}$ is a finite state space of cardinality $k$, such that $\mathcal{S} = \{ s_1, s_2, \ldots , s_k \}$. 
	We call $\{ X_n, n \in \mathbb{N}_0\}$ a Markov chain if 
	\[ \mathbb{P}(X_{n} = s_{n} | X_{n-1} = s_{n-1}, X_{n-2} = s_{n-2} , \ldots , X_0 = s_0 ) = \mathbb{P}(X_{n} = s_{n} | X_{n-1} = s_{n-1}) \]
	for all $n \in \mathbb{N}_0$ and all $s_0, s_1, \ldots , s_{n} \in \mathcal{S}$ whenever the two conditional probabilities are well defined, this is, when $\mathbb{P}(X_{n} = s_{n}, X_{n-1} = s_{n-1}, \ldots , X_0 = s_0) > 0$. 
\end{definition}

Intuitively, this means that the next state in the chain is only dependent on the current state of the chain, and that the history of how the chain got to the current state is irrelevant to the next step. 
This behaviour is known as the Markov property, and one very valuable insight that emerges from this behaviour is being able to completely determine all properties of the chain given only the initial state of it and the knowledge of how the chain behaves at all of its possible states. 

The initial state or initial probability is self-explanatory, it tells us the expected behaviour of the first element in the chain, $X_0$. Formally, we call $\mu^{(0)} = \{ \mu_s^{(0)} : s \in \mathcal{S} \}$ with $\mu_s^{(0)} = \mathbb{P}(X_0 = s)$ the initial probability distribution of the process. 

% Duda
The other important component to fully describe a Markov chain is the transition probabilities from one state $s_i$ to another $s_j$ for all $s_i, s_j \in \mathcal{S}$. We identify this probability as 
\[ p_{i,j} = \mathbb{P}(X_{n} = s_j | X_{n-1} = s_i). \]
If we take all combinations of the states the chain can be in and map them to all other states, we get what is known as the transition matrix $P$, where each entry in the matrix denotes the probability of moving from one specific state to another. This matrix, because it is fundamentally a matrix made up of probabilities, has two interesting properties for the purposes of this work: 
\begin{itemize}
	\item $p_{i,j} \geq 0$ for all $i,j \in \{ 1, 2, \ldots, n \}$
	\item $\sum_{i=1}^{n} p_{i,j} = 1$ for all $j \in \{ 1, 2, \ldots, n \}$.  
\end{itemize}
We note that these two properties are the definition of a stochastic matrix, and therefore, by construction, Markov chain transition matrices are stochastic matrices. We have only described that we can fully determine the Markov chain from its initial state and its transition probabilities, but we have not shown this concept concretely. The following theorem ties these loose ends together and formally describes this property.

\begin{theorem}
	Let $\{ X_n, n \in \mathbb{N}_0 \}: \Omega \mapsto \mathcal{S}$ be a stochastic process defined on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ with $\mathcal{S}$ a finite state space. Then, $\{ X_n, n \in \mathbb{N}_0 \}$ is a Markov chain if and only if there exists a stochastic matrix $P$ such that 
	\[ \mathbb{P}(X_n = s_n | X_{n-1} = s_{n-1}, \ldots , X_0 = s_0) = p_{n-1, n} \]
	for all $n \in \mathbb{N}_0$ and all $s_0, s_1, \ldots , s_n \in \mathcal{S}$ such that $\mathbb{P}(X_{n-1} = s_{n-1}, \ldots , X_0 = s_0) > 0$. 
\end{theorem}

This theorem is of great importance in the area of study of Markov chains, and we refer the interested reader to \cite[\S 1.2]{chung_markov_1960} for a detailed proof. this theorem establishes a bridge between the theory of Markov processes in general and the properties of linear algebra and matrix theory. One such interesting property is the use of eigenvectors and eigenvalues of the transition matrix used to define a stationary distribution, which is what we are ultimately building towards this section. 

% Duda
We have seen that the transition matrix $P$ governs the behaviour and the changes that happen from one state of the Markov chain to the next. 
A natural follow up question arises from trying to understand the behaviour of the chain an arbitrary $k > 1$ steps in the future instead of a single step, such as if we wanted to calculate $\mathbb{P}(X_{i+k} = s_{i+k} | X_i = s_i)$. 
We first note that this multi-step transition probability can be simplified, assuming that the Markov chain is homogenous, to the following: 
\[ \mathbb{P}(X_{i+k} = s_{i+k} | X_i = s_i) = \mathbb{P}(X_{k} = s_{k} | X_0 = s_0) \]
which we denote as $p_{i,k}^{(k)}$. Because we can define the individual probabilities of transitioning from one step to another after $k$ steps, it stands to reason that there would be a transition matrix to describe this behaviour.
The theorem that establishes the way these relationships work is the Chapman-Kolmogorov theorem and equation, which we present now. The formulation of this theorem presented in this work is taken from \cite[\S 8.3]{Zitkovic_2010}. 

\begin{theorem}[Chapman-Kolmogorov Theorem]
	Let $n,m \in \mathbb{N}_0$ and $s_i, s_j \in \mathcal{S}$. Then, $p_{i,j}^{(n+m)}$, which is the probability that starting from state $i$ at step $n$ we end up in state $j$ after $m$ steps is given by: 
	\[ p_{i,j}^{(n+m)}  = \sum_{k \in \mathcal{S}} p_{i,k}^{(n)} \, p_{k,j}^{(m)}. \]
\end{theorem} 

% Duda
\begin{proof}
	We will prove the case when the Markov process is homogenous. We begin by noting that $p_{i,j}^{(n+m)} = \mathbb{P}(X_{n+m} = s_j | X_n = s_i) = \mathbb{P}(X_m = s_j | X_0 = s_i) = p_{i,j}^{(m)}$. Applying the law of total probability and using the Markov property, 
	\begin{align*} 
		p_{i,j}^{(n+m)} &= \\ 
		&= \mathbb{P}(X_{n+m} = s_j | X_n = s_i) \\
		&= \mathbb{P}(X_{m} = s_j | X_0 = s_i) \\
		&= \sum_{k \in \mathcal{S}} \frac{\mathbb{P}(X_m = s_j, X_{n} = s_k, X_0 = s_i)}{\mathbb{P}(X_0 = s_i)} \\
		&= \sum_{k \in \mathcal{S}} \mathbb{P}(X_m = s_j | X_n = s_k) \mathbb{P}(X_n = s_k | X_0 = s_i) \\
		&= \sum_{k \in \mathcal{S}} p_{i,k}^{(n)} \, p_{k,j}^{(m-n)}. 
	\end{align*}
\end{proof}

We now introduce the following lemma that takes advantage of this property to provide an alternative way of computing and understanding the transition matrix $P$. 

\begin{lemma}
	Let $P^k$ be the $k$-th order matrix power of the transition matrix $P$ for a homogeneous Markov chain $\{ X_n, n \in \mathbb{N}_0 \}$. Then, 
	\[ p_{i,j}^{(k)} = (P^k)_{i,j} \text{ for } i,j \in \mathcal{S}. \]
\end{lemma}

There is a way to prove this lemma by induction directly, but we opt instead to use the Chapman-Kolmogorov Equation n-1 times to achieve the final result, as follows. 

This lemma becomes useful in proving the following theorem. 

\begin{theorem}
	Let $n,m \in \mathbb{N}_0$ and $i,j \in \mathcal{S}$. Then, $p_{i,j}^{(n+m)}$, which is the probability that starting from state $i$ in step $n$ we end up in state $j$ after $m$ steps is given by: 
	\[ p_{i,j}^{(n+m)} = \sum_{k \in \mathcal{S}} p_{i,k}^{(m)} \cdot p_{k,j}^{(n)}. \]
\end{theorem}

From here, we now present two definitions relating to the properties of matrices which will become important when further exploring the types of Markov chains we are interested in. 

\begin{definition}
	A matrix $A \in \mathbb{R}^{n \times m}$ is called non-negative if $a_{i,j} \geq 0$ for all $1 \leq i \leq n$ and $1 \leq j \leq m$. 
\end{definition}

Before continuing, we note that with stochastic matrices being just glorified assortments of probabilities that are neatly ordered, they are always non-negative. 

\begin{definition}
	Let $A$ be a non-negative matrix. If there exists $N_0 \geq 1$ such that all elements of $A^{n_0}$ are positive, then we call $A$ a quasi-positive matrix. 
\end{definition}

These two definitions are necessary building blocks to get to the property we are truly interested in, ergodicity. 

\begin{definition}
	A Markov chain $\{ X_n: n \in \mathbb{N}_0 \}$ is called ergodic if the limit $\pi_j \coloneqq \lim_{n \rightarrow \infty} p_{i,j}^{(n)}$
	\begin{itemize}
		\item Exists for all $j \in \{1,2,\ldots,k\}$. 
		\item Is positive and does not depend on $i$. 
		\item $\pi = (\pi_1 , \pi_2 , \ldots , \pi_k)$ is a probability distribution on $\mathcal{S}$. 
	\end{itemize}
\end{definition}

Ergodicity is an incredibly powerful property that is not native to stochastic processes per say, but rather it is inherited from measure theory. In a sense, for a system to be ergodic it needs to be governed by constant or unchanging probability laws, irregardless of a change in state, or in the context of time-centric processes, changes in time. As a simple example, we consider throwing a fair coin in the air 100 times. We would (on average) get the same result if 100 people throw one coin compared to one person throwing the coin 100 times. This would be an ergodic system. On the other hand, if we change the coin to one that is imbalanced or unfair after 50 throws are completed, then the laws governing the change themselves would not remain constantas time passes or we change states, and this would be an example of a non-ergodic system. 

The scope of this work does not cover this theory in as much detail as the author would like, but the interested reader is directed to sections 24 and 36 of \cite{billingsley2012probability} for a much more complete rundown of Ergodicity looked at through the lenses of measure theory and stochastic processes. The result ergodicity provides that is the foundation for MCMC applications is the following theorem. 

\begin{theorem}
	Let $\{ X_n: n \in \mathbb{N}_0\}$ be an ergodic Markov chain, such that 
	\[ \pi_j = \lim_{n \rightarrow \infty} p_{i,j}^{(n)}, \text{ for all } j \in \mathcal{S}. \]
	Then, the vector $\pi$ is a unique solution of 
	\[ \pi^T = \pi^T P \]
	further, $\pi$ is a probability distribution on $\mathcal{S}$. 
\end{theorem}

\begin{proof}
	Let $j$ be an arbitrary member of $\mathcal{S}$. Then, 
	\begin{align*}
		\pi_j &= \lim_{n \rightarrow \infty} p_{i,j}^{(n)} \\
		&= \lim_{n \rightarrow \infty} p_{i,j}^{(n+1)} \\
		&= \lim_{n \rightarrow \infty} p_{i,j}^{(n)} \cdot p_{i,j} \\
		&= \pi_j \cdot p_{i,j}.
	\end{align*}
	Since this hods true for all $j \in \mathcal{S}$, then in particular it holds true for the vectorized version of this equation, this being 
	\[ \begin{bmatrix}
		\pi_1 \\
		\pi_2 \\
		\vdots \\
	\end{bmatrix} = 
	\begin{bmatrix}
		\pi_1, &
		\pi_2, &
		\ldots 
	\end{bmatrix} 
	\begin{bmatrix}
		p_{i,1} \\
		p_{i,2} \\
		\vdots \\
	\end{bmatrix} \]
Which is equivalent to saying that $\pi^T = \pi^T P$.
\end{proof}

If we construct a Markov chain in a way that it is ergodic an control the distribution $\pi$ so that it is something that we want, we can guarantee that after a sufficiently large amount of iterations $n$ we will be sampling from the stationary distribution of the process, which is invariant through time because of the property presented in the above theorem. This is the crux of understanding the Markov Chain Monte Carlo technique which is the central object of study of this section. Delving into the construction of the Metropolis-Hastings algorithm and the fundamental building blocks needed to construct these chains that eventually lead to interesting stationary distributions would make this work exceedingly long, but \cite{rubinstein2016simulation} and \cite{robert1999monte} are good resources for a detailed rundown of these topics. We will return much later in this work when discussing the BASS method to MCMC and why it is so essential to this process. We will not look at pure MCMC though but an interesting modification to the base algorithm known as the Reversible Jump Markov Chain Monte Carlo (RJMCMC) algorithm. 

For now though we move on to the other interesting sub-case of stochastic processes relevant to this work, the continuous-time Gaussian Process. 

\section{Gaussian Processes}

A Gaussian process can be thought of as a generalization of the Gaussian (normal) distribution when viewed from the lens of stochastic processes. Stochastic processes, as we saw before, are indexed by a parameter, which itself belongs to an index set. The Gaussian process extends the definition of the Gaussian distribution from being valued on scalars or vectors to a more general index set of the form $T \subseteq \mathbb{R}^+$. Note that this set can contain an infinite number of points, which is the main difference between the Gaussian distribution and the Gaussian process. This extension is not trivial though, and to specify the structure of these particular types of stochastic processes, we aid ourselves with finite dimensional distributions. We now provide a formal definition of these types of distributions. 

\begin{definition}
	Let $\{ t_i \}_{i \leq k}$ be a sequence of points\footnote{Sometimes referred to as time points, considering this index not as an abstract set but a representation of the progress of time as indexed by a subset of the positive reals.} such that $\{ t_i \}_{i \leq k} \subseteq T$. The $k$-dimensional random vector $(X_{t_1}, X_{t_2}, \ldots, X_{t_k})$ has a distribution $\mu_{t_1, t_2, \ldots, t_k}$ over $\mathbb{R}^k$. These measures $\mu_{t_1, t_2, \ldots, t_k}$ are the finite dimensional distributions of the process. 
\end{definition}

One logical follow up is to ask if the inverse of this definition is also true. This would be, starting from a consistent\footnote{Consistency is a fuzzy term with a lot of meanings, but its meaning in this context will become clear with the theorems that follow.} joint distribution of the random variables $(X_{t_1}, X_{t_2}, \ldots, X_{t_k})$ , with $k \in \mathbb{N}$, then would it be possible to define a probability measure $\mu_{t_1, t_2, \ldots, t_k}$ on a higher cardinality set, in this case $\mathbb{R}^k$ that matches these distributions exactly? The positive answer to this question is the Kolmogorov Extension Theorem, an incredibly deep and powerful result that guarantees the existence of stochastic processes by only defining a finite set of distributions that share some consistency requirements. This particular formulation of the theorem is taken from \cite{enwiki:1176088789}. 

\begin{theorem}[Kolmogorov Existence Theorem]
	Let $T$ denote some interval, and let $n \in \mathbb{N}$. For each $k \in \mathbb{N}$ and finite sequence of distinct times $t_1, \ldots, t_k \in T$, let $\nu_{t_1 \ldots t_k}$ be a probability measure on $(\mathbb{R}^n)^k$. Suppose that these measures satisfy two consistency conditions:
	
	\begin{enumerate}
		\item For all permutations $\pi$ of $\{1, \ldots, k\}$ and measurable sets $F_i \subseteq \mathbb{R}^n$,
		\[
		\nu_{t_{\pi(1)}\ldots t_{\pi(k)}}\left(F_{\pi(1)} \times \ldots \times F_{\pi(k)}\right) = \nu_{t_1 \ldots t_k}\left(F_1 \times \ldots \times F_k\right);
		\]
		\item For all measurable sets $F_i \subseteq \mathbb{R}^n$, $m \in \mathbb{N}$,
		\[
		\nu_{t_1 \ldots t_k}(F_1 \times \ldots \times F_k) = \nu_{t_1 \ldots t_k, t_{k+1}, \ldots, t_{k+m}}\left(F_1 \times \ldots \times F_k \times \underbrace{\mathbb{R}^n \times \ldots \times \mathbb{R}^n}_m\right).
		\]
	\end{enumerate}
	
	Then there exists a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and a stochastic process $X: T \times \Omega \to \mathbb{R}^n$ such that
	\[
	\nu_{t_1 \ldots t_k}(F_1 \times \ldots \times F_k) = \mathbb{P}(X_{t_1} \in F_1, \ldots, X_{t_k} \in F_k)
	\]
	for all $t_i \in T$, $k \in \mathbb{N}$, and measurable sets $F_i \subseteq \mathbb{R}^n$, i.e., $X$ has $\nu_{t_1 \ldots t_k}$ as its finite-dimensional distributions relative to times $t_1 \ldots t_k$.
	
\end{theorem}

The Kolmogorov Extension Theorem addresses the question of whether we can construct a stochastic process given only a finite set of joint distributions. These joint distributions need to satisfy a consistency condition, which means that any finite subset of random variables should have joint distributions that are compatible with each other.

The theorem states that if such consistent joint distributions exist, then it is possible to define a stochastic process on a higher-dimensional space, often denoted as $\mathbb{R}^k$, such that the original given distributions match the marginal distributions of this process. In essence, the Kolmogorov Extension Theorem provides a way to create a stochastic process that captures the dependencies and correlations between random variables at different time points, even when we only have limited information about their joint distributions. 

In the case of Gaussian processes, this means that if we have a finite set of random variables following consistent joint Gaussian distributions, we can construct a Gaussian process on a continuous domain (typically a subset of $\mathbb{R}$) such that any finite subset of the process is jointly Gaussian with the desired covariance structure. 

With this auxiliary distribution out of the way, we can now proceed to define the Gaussian process formally. 

\begin{definition}
	Let $\{ X_t: t \in \mathbb{R}^+ \}$ be a continuous-time stochastic process. We call $\{ X_t: t \in \mathbb{R}^+ \}$ a Gaussian process if for every set $\{ t_i \}_{i \leq k}$ and every finite $k$, the finite-dimensional distribution of the $k$-dimensional random vector $(X_{t_1}, X_{t_2}, \ldots, X_{t_k})$ has a Gaussian multivariate distribution when $k>1$ and a Gaussian univariate distribution when $k=1$. 
\end{definition} 



An important extension of this is that we can consider the Gaussian process a function $f$ and evaluate it at certain points $x$, where $x \in \mathcal{X}^\Theta$, the input space. This is, for each $x$, there is a random variable $f(x)$ that represents the possible output of the function $f$ at this location, or a realization of a certain Gaussian distribution at location $x$. This can also be understood as taking a sample from the distribution $f(x)$. As with any version of a Gaussian distribution, $f(x)$ can be completely be defined by it's mean and variance. Unlike the finite-dimensional cases though, by definition we can have infinitely many values in the index space $T$, and therefore the reasonable approach to defining the mean and variance of functions of elements in the index space. We denote the mean function of the process as: 
\[ m(x) = \mathbb{E}[f(x)] \]
and the covariance function as: 
\[ k(x, x') = \mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]. \]

The main reason why the covariance function is denoted by $k$ is that this name alludes to the fact that $k$ is a kernel of the terms in the input space. We now present a formal definition of this type of function, followed by some intuition on its interpretation. With these definitions though, we write the distribution function for $f$ as 
\[ f(x) \sim \mathcal{GP}(m(x), k(x,x')). \]

\begin{definition}
	Let $\mathcal{X}$ be an input space like the ones we have previously defined (in the case of Gaussian processes, $\mathcal{X} = \{ X_t : t \in T \subseteq \mathbb{R}^+ \}$), and let $V$ be a vector space equipped with an inner product. Also, let $\varphi$ be a mapping such that $\varphi: \mathcal{X} \times \mathcal{X} \rightarrow V$. We call the function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ a kernel or kernel function when $k(x, x') = \langle \varphi (x), \varphi (x') \rangle_V$. 
\end{definition}

What this kernel function allows us to do is basically borrow an inner product from a different space, in this definition $V$, and apply it to the transformed elements of our input or feature space. This allows us to cheat in some regard and compute dot products or metrics in the input space without knowing or needing to know the properties of that space. Tying back the kernel to Gaussian processes, we note that while the mean only controls the average value across realizations of the process $f$, the covariance or kernel function actually controls the underlying behavior of the realizations or Gaussian processes. As an example of this, we borrow the following figure from \cite{ghahramani2011tutorial}. 

\begin{figure}[h]
	\includegraphics[width=4cm]{Figures/missing.png}
	\centering
	\label{trajectories}
\end{figure}

Basically, all major properties such as smoothness, differentiability, sparcity and range (to name only a few) are controlled by the choice of the kernel function. 

While the kernel function can be basically anything that fulfils the above definition, there are some particularly useful or popular kernel functions when dealing with Gaussian processes. Chief among them is the squared exponential kernel, also known as the radial basis function. This kernel is defined as
\[ k(x, x') = \sigma_f^2 \exp \left( \frac{- \ell (x - x')}{2} \right), \]
where $\sigma_f^2$ and $\ell$ are hyperparameters. While we do not go into too much detail as to why this is a good choice for a kernel, we refer the interested reader to \cite[\S 2.1]{garnett_bayesoptbook_2023}. 

The hyperparameters defined previously each control the general behaviour of the process when fitting data points. A more detailed run down of these points is given in the next section, which talks about Gaussian process regression. This is the method by which uncertainty is measured and function approximations are conducted in the traditional approach to Bayesian optimization. 

\section{Gaussian Process Regression}

One of the applications of Gaussian processes is to use them as a method to perform supervised learning using it. The method generates from data a mean function and then uses that mean function to extrapolate or predict on points where there is no data observed. As we described in the last chapter, we can fully specify or determine a particular Gaussian process by specifying its mean $m(x) = \mathbb{E}[f(x)]$ and its kernel function $k(x,x')$. While in the last chapter we started with the Gaussian process and from that generate the values on the rest of the input space, in this chapter we are going to be doing somewhat of the inverse of that. We will be starting from a set of data $\{ x_i \}_{i \leq n} = \mathcal{D} \subset \mathcal{X}$, and adjusting the properties of the kernel and mean to achieve the best fit Gaussian process to the points and minimize the error. 

One of the advantages of Gaussian process regression over other forms of supervised learning algorithms (such as linear regression) is the fact that Gaussian processes are a non-parametric model. Without delving too much into the specifics, this means that the number of parameters controlling the model is not fixed at a certain quantity, but rather is adaptive. The amount of parameters can grow and shrink depending on the data that is being fit. 

This however does not mean that we do not have control over the behavior of the function we decide to adjust to the points. In the following figure, we can see how changing the hyperparameter $\ell$ in the squared exponential covariance function changes the relative smoothness of the fit function. 

\begin{figure}[h]
	\includegraphics[width=4cm]{Figures/missing.png}
	\centering
	\caption{Gaussian Processes for ML, page 20.}
	\label{trajectories}
\end{figure}

We also note how radically the confidence intervals change from one adjustment to the next based on moving this hyperparameter around. Because we can define an infinite amount of Gaussian processes based on our choice of hyperparameters (and further even by changing the kernel function), \textcolor{red}{HOW TO STATE THIS AS A MINIMIZATION PROBLEM?}
This problem translates to finding under the given assumptions the function that best adjusts to these points in the sense that it minimizes the uncertainty present. This will involve the selection of the best\footnote{Best is a dangerous word in this context. Obviously depending on the problem and the minimization objectives, this best set of hyperparameters that define this function can change, but here we simply refer to the one or family of functions that solves the above minimization problem.} hyperparameters given the constraints of the problem and using that defined mean function to predict values not in the data set.

To get there, we will begin by going over regression in general, then make the link to the formal theory of Gaussian process regression to show how supervised learning can be performed by this method. 

\subsection{Preamble to Gaussian Process Regression: A Rehash on Regression Analysis}

The central task of this section is regression, which is the process of estimating or generating understanding of a theoretical function $f: \mathcal{X} \rightarrow \mathbb{R}$ that describes the relationship between a response variable $y$ and a group or single input variable described by the vector $\boldsymbol{x}$. While we do not know the exact behavior of $f$ the function that describes this relationship\footnote{We will discuss this in later chapters, but this is actually one of the greatest advantages of Bayesian Optimization, in that in can work on so called \textit{black-box} functions}, what we do have is a set of observations $\mathcal{D} \subset \mathcal{X}$, where $\mathcal{D} = \{ (x_i, y_i), i = 1,2,\ldots, n \}$ and $x_i \in \mathbb{R}^d$ and $y_i \in \mathbb{R}$. These observations constitute point-wise realizations of the function $f$. From that, for a set of new unknown points $\boldsymbol{x}_*$ we want to predict or infer the quantity $f(x_{i*}) = y_*$, which would be point-wise estimates of these unseen or unknown realizations of $f$. 

One example of another method od solving this problem is the well known and well loved linear regression, that assumes that the function $f$ is of the general form 
\[ y = f(x) = x^T \beta + \epsilon \]
where $\epsilon \sim \mathcal{N}(0, \sigma^2)$. The error variance $\sigma^2$ and the coefficients $\beta$ are generated by a process of minimizing the error of the outputs generated. 

One of the disadvantages of these types of methods is that they only give one function as the output, but there may well be an infinite number of functions that fit a certain set of points. As an example, consider the following figure. 

\begin{figure}[h]
	\includegraphics[width=4cm]{Figures/missing.png}
	\centering
	\caption{Gaussian Processes for ML, page 20.}
	\label{trajectories}
\end{figure}

Notice how for the training dataset $\mathcal{D}$, all of these functions have error zero! The only recourse we have is to restrict these functions to generate one function (or at least a finite family of functions), and not an infinity of them. As with many other areas of mathematics, there is a trade off that we have to choose to perform between how general a solution to a problem is and how many solutions to that problem we wish to get.

The key insight to understanding Gaussian process regression from a statistician's point of view is changing the way we think about regression. Instead of going the traditional way and providing restrictions to generate particular approximations, why don't we consider the space of possible functions that can fit these points as coming from a probability model? What if the observed functions in the past figure are realizations of a more general class of random variable that is coherent in the function space, rather than the discrete variable or vector space? From this groundwork is where Gaussian processes come into the picture. The only added restriction that we impose on the functions is that as a family, they come from a generalization to infinite dimensions of the multivariate normal distribution, and with that, we generate the entire conceptual framework from which we derive and generate the Gaussian process regresion method. Having now set the expectation of what we wish to accomplish, we can now construct the theory of this different class of regression analysis. 

\subsection{Theory of Gaussian Process Regression (from the function space view)}

This section of the text is heavily based on the work and structure of \cite{rasmussen2006gaussian} and to a much lesser extent \cite{wang2020intuitive}. The book Gaussian Processes for Machine Learning is the go-to reference and source for modern applications of Gaussian processes in general and it would be a disservice to the reader to not follow that structure and argumentation. 

We can split up the set $\mathcal{D}$ into training and test, defining $\mathcal{D}_{train} = \{ x_i , y_i \}_{i=1}^n$, and $\mathcal{D}_{test} = \{ x^*_i , y^*_i \}_{i=1}^{^*}$, with $x_i, x^*_i \in \mathbb{R}^d$ and $y_i, y^*_i \in \mathbb{R}$. As described before, what we want to achieve is basically generating out of sample predictions for the test set $D_{test}$. Since the function $f = m$ defined is the mean of the process we defined before, we can simply evaluate the out of sample points $x_i^*$ on $f$ and we have an estimate for $y_i^*$. For simplicity and by convention, we denote the set of of test outputs $\boldsymbol{f}_*$. Because of the structure we have defined, formally, the regression function $f$ is really a normal distribution when conditioned by the training dataset $\mathcal{D}_{train}$, which would be
\[ \mathbb{P}(\boldsymbol{f} | \boldsymbol{X}) = \mathcal{N}(f | \mu, k) \]
where $\boldsymbol{X} = [ x_1, x_2, \ldots , x_n ]$, $\boldsymbol{f} = [ f(x_1), f(x_2), \ldots, f(x_n) ]$, $\mu= [ m(x_1), m(x_2), \ldots, m(x_n) ]$ and $k$ the kernel or covariance function of the process. By convention, it is customary to set $m(\boldsymbol{X}) = 0$, but there is a reasoning behind this. When dealing with the type of data generally modelled by these types of functions, it is common (encouraged even) to normalize the data before applying these methods to it. Another important piece of shorthand notation we will use is $K=k(\boldsymbol{X}, \boldsymbol{X})$, $K_*=k(\boldsymbol{X}, \boldsymbol{X}_*)$, and $K_{**}=k(\boldsymbol{X}_*, \boldsymbol{X}_*)$. Note that since the number of training points need not equal the number of test points, then $n$ has not attributable relationship with $n^*$ and we cannot state any properties of the matrix $K_*$, much less say that it is a square matrix. 

Since we have formally defined the distribution of $\boldsymbol{f}$ and we know that $\boldsymbol{f}_*$ is the same distribution but considering a different set of data points $\boldsymbol{X}_* = [x^*_1, \ldots, x^*_n]$, we can define the joint distribution of these vectors as 
\begin{equation}
	\label{joint_dist}
	\begin{bmatrix}
		\boldsymbol{f} \\
		\boldsymbol{f}_*
	\end{bmatrix} \sim \mathcal{N}\left( \begin{bmatrix}
		0 \\
		0
	\end{bmatrix} , \begin{bmatrix}
		K & K_* \\
		K_*^T & K_{**}
	\end{bmatrix} \right), 
\end{equation}

which is the joint probability distribution $\mathbb{P}(\boldsymbol{f}, \boldsymbol{f}_* | \boldsymbol{X}, \boldsymbol{X}_*)$. This is a massive step in constructing the regression model but we are still missing one component. When dealing with regression and prediction of values outside of the original training dataset, we do not need the joint probability, but the conditional probability of the test set given everything else we know. Formally, this means that we need to find the probability distribution of $\mathbb{P}(\boldsymbol{f}_* | \boldsymbol{X}, \boldsymbol{X}_*, \boldsymbol{f})$. Luckily, since we are dealing with Gaussian distributions, there is a very neat closed form expression that describes this probability. Referring to section $A.2$ of \cite{rasmussen2006gaussian}, and noting that the mean for both $\boldsymbol{f}$ and $\boldsymbol{f}_*$ is 0, then 
\[ \boldsymbol{f}_* | \boldsymbol{f} \sim \mathcal{N}(K_*^T K^{-1} \boldsymbol{f}, K_{**} - K_*^T K^{-1} K_*). \]

This probability function is what is used to calculate out of sample predictions for the out of sample data $\boldsymbol{X}_*$. 

\subsubsection{Gaussian Process Regression Over Noisy Observations}

When there is inherent randomness or noise in the signals being calculated, the structure of the problem itself changes. If we can assume that the noisy signals we are receiving are of the form $y = f(\boldsymbol{x}) + \varepsilon$ with the noise or inherent randomness captured by $\varepsilon$ and these being independent, identically distributed observations of a normal or Gaussian distribution, the covariance of the observations therefore becomes 
\[ \text{cov}(\boldsymbol{y}) = K(\boldsymbol{X}, \boldsymbol{X}) + \sigma^2 I, \]
with $I$ the identity matrix. In practical terms this means that the inherent variance of every observation is present as an added term on the diagonal of the covariance matrix (this is because of the assumption that the noise is independent). This change causes the joint distribution originally defined in equation \ref{joint_dist} to
\[ 
\begin{bmatrix}
	\boldsymbol{y} \\
	\boldsymbol{f}_*
\end{bmatrix} \sim \mathcal{N}\left( \begin{bmatrix}
	0 \\
	0
\end{bmatrix} , \begin{bmatrix}
	K + \sigma^2 I & K_* \\
	K_*^T & K_{**}
\end{bmatrix} \right), 
\]

and the conditional distributions for prediction are therefore 
\begin{align*}
	\boldsymbol{f}_* | \boldsymbol{X}, \boldsymbol{y}, \boldsymbol{X}_* &\sim \mathcal{N}(\bar{\boldsymbol{f}_*}, \text{cov}(\boldsymbol{f}_*)), \text{ where} \\
	\bar{\boldsymbol{f}_*} &\coloneqq \mathbb{E}[\boldsymbol{f}_* | \boldsymbol{X}, \boldsymbol{y}, \boldsymbol{X}_*] = K_*^T [K + \sigma^2 I]^{-1} \boldsymbol{y} \\
	\text{cov}(\bar{\boldsymbol{f}_*}) &= K_{**} -  K_*^T [K + \sigma^2 I]^{-1} K_*.
\end{align*}

These equations are for the entire training and test set though. For a test point $x_*$, we can define the vector $\boldsymbol{k}_*$ as the vector of covariances between this new test point and each of the points in the training dataset. This would be equivalent to taking the column associated with that single point from the test data matrix $K_*$. If we wanted just the closed form for this single out of sample observation, that would be 
\begin{align*}
	\bar{f_*} &= \boldsymbol{k}_*^T(K + \sigma^2 I)^{-1} \boldsymbol{y}, \\
	\mathbb{V}(\bar{f_*}) &= k(x_*, x_*) - \boldsymbol{k}_*^T (K + \sigma^2 I)^{-1} \boldsymbol{k}_*.
\end{align*}

\textcolor{red}{close this chapter off properly.}




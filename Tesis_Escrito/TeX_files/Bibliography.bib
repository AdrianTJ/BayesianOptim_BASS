 @inproceedings{Bergstra_Yamins_Cox_others_2013, title={Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms}, volume={13}, booktitle={Proceedings of the 12th Python in science conference}, publisher={Citeseer}, author={Bergstra, James and Yamins, Dan and Cox, David D and others}, year={2013}, pages={20} }
 
 
 @article{Bergstra_Bardenet_Bengio_Kégl, title={Algorithms for Hyper-Parameter Optimization}, abstractNote={Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.}, author={Bergstra, James S and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs}, pages={9}, language={en} }
 
 
 @inproceedings{Daxberger_Makarova_Turchetta_Krause_2020, title={Mixed-Variable Bayesian Optimization}, url={http://arxiv.org/abs/1907.01329}, DOI={10.24963/ijcai.2020/365}, abstractNote={The optimization of expensive to evaluate, black-box, mixed-variable functions, i.e. functions that have continuous and discrete inputs, is a difficult and yet pervasive problem in science and engineering. In Bayesian optimization (BO), special cases of this problem that consider fully continuous or fully discrete domains have been widely studied. However, few methods exist for mixed-variable domains and none of them can handle discrete constraints that arise in many real-world applications. In this paper, we introduce MiVaBo, a novel BO algorithm for the efficient optimization of mixed-variable functions combining a linear surrogate model based on expressive feature representations with Thompson sampling. We propose an effective method to optimize its acquisition function, a challenging problem for mixed-variable domains, making MiVaBo the first BO method that can handle complex constraints over the discrete variables. Moreover, we provide the first convergence analysis of a mixed-variable BO algorithm. Finally, we show that MiVaBo is significantly more sample efficient than state-of-the-art mixed-variable BO algorithms on several hyperparameter tuning tasks, including the tuning of deep generative models.}, note={arXiv:1907.01329 [cs, stat]}, booktitle={Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence}, author={Daxberger, Erik and Makarova, Anastasia and Turchetta, Matteo and Krause, Andreas}, year={2020}, month={Jul}, pages={2633–2639} }
 
 
 @article{Denison_Mallick_Smith_1998, title={Bayesian MARS}, volume={8}, ISSN={09603174}, DOI={10.1023/A:1008824606259}, number={4}, journal={Statistics and Computing}, author={Denison, D. G. T. and Mallick, B. K. and Smith, A. F. M.}, year={1998}, pages={337–346} }
 
 
 @article{Francom_Sansó_2020, title={BASS : An R Package for Fitting and Performing Sensitivity Analysis of Bayesian Adaptive Spline Surfaces}, volume={94}, ISSN={1548-7660}, url={http://www.jstatsoft.org/v94/i08/}, DOI={10.18637/jss.v094.i08}, number={8}, journal={Journal of Statistical Software}, author={Francom, Devin and Sansó, Bruno}, year={2020}, language={en} }
 
 
 @article{Frazier_2018, title={A Tutorial on Bayesian Optimization}, url={http://arxiv.org/abs/1807.02811}, abstractNote={Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.}, note={arXiv:1807.02811 [cs, math, stat]}, number={arXiv:1807.02811}, publisher={arXiv}, author={Frazier, Peter I.}, year={2018}, month={Jul} }
 
 
 @article{Friedman_1991, title={Multivariate Adaptive Regression Splines}, volume={19}, ISSN={0090-5364}, url={https://projecteuclid.org/journals/annals-of-statistics/volume-19/issue-1/Multivariate-Adaptive-Regression-Splines/10.1214/aos/1176347963.full}, DOI={10.1214/aos/1176347963}, number={1}, journal={The Annals of Statistics}, author={Friedman, Jerome H.}, year={1991}, month={Mar} }
 
 
 @article{Garrido-Merchán_Hernández-Lobato_2020, title={Dealing with Categorical and Integer-valued Variables in Bayesian Optimization with Gaussian Processes}, volume={380}, ISSN={09252312}, DOI={10.1016/j.neucom.2019.11.004}, abstractNote={Bayesian Optimization (BO) methods are useful for optimizing functions that are expen- sive to evaluate, lack an analytical expression and whose evaluations can be contaminated by noise. These methods rely on a probabilistic model of the objective function, typically a Gaussian process (GP), upon which an acquisition function is built. The acquisition function guides the optimization process and measures the expected utility of performing an evaluation of the objective at a new point. GPs assume continous input variables. When this is not the case, for example when some of the input variables take categorical or integer values, one has to introduce extra approximations. Consider a suggested input location taking values in the real line. Before doing the evaluation of the objective, a common approach is to use a one hot encoding approximation for categorical variables, or to round to the closest integer, in the case of integer-valued variables. We show that this can lead to problems in the optimization process and describe a more principled approach to account for input variables that are categorical or integer-valued. We illustrate in both synthetic and a real experiments the utility of our approach, which significantly improves the results of standard BO methods using Gaussian processes on problems with categorical or integer-valued variables.}, note={arXiv:1805.03463 [cs, stat]}, journal={Neurocomputing}, author={Garrido-Merchán, Eduardo C. and Hernández-Lobato, Daniel}, year={2020}, month={Mar}, pages={20–35} }
 
 
 @article{Green_1995, title={Reversible jump Markov chain Monte Carlo computation and Bayesian model determination}, volume={82}, ISSN={0006-3444, 1464-3510}, DOI={10.1093/biomet/82.4.711}, number={4}, journal={Biometrika}, author={Green, Peter J.}, year={1995}, pages={711–732}, language={en} }
 
 
 @article{Klein_Falkner_Bartels_Hennig_Hutter_2017, title={Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets}, url={http://arxiv.org/abs/1605.07079}, abstractNote={Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed Fabolas, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that Fabolas often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.}, note={arXiv:1605.07079 [cs, stat]}, number={arXiv:1605.07079}, publisher={arXiv}, author={Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank}, year={2017}, month={Mar} }
 
 
 @inbook{Luong_Gupta_Nguyen_Rana_Venkatesh_2019, address={Cham}, series={Lecture Notes in Computer Science}, title={Bayesian Optimization with Discrete Variables}, volume={11919}, ISBN={978-3-030-35287-5}, url={http://link.springer.com/10.1007/978-3-030-35288-2_38}, DOI={10.1007/978-3-030-35288-2_38}, booktitle={AI 2019: Advances in Artificial Intelligence}, publisher={Springer International Publishing}, author={Luong, Phuc and Gupta, Sunil and Nguyen, Dang and Rana, Santu and Venkatesh, Svetha}, editor={Liu, Jixue and Bailey, James}, year={2019}, pages={473–484}, collection={Lecture Notes in Computer Science}, language={en} }
 
 
 @book{Ross_1996, address={New York}, edition={2nd ed}, series={Wiley series in probability and statistics}, title={Stochastic processes}, ISBN={978-0-471-12062-9}, callNumber={QA274 .R65 1996}, publisher={Wiley}, author={Ross, Sheldon M.}, year={1996}, collection={Wiley series in probability and statistics} }
 
 
 @article{Snoek_Larochelle_Adams_2012, title={Practical Bayesian Optimization of Machine Learning Algorithms}, url={http://arxiv.org/abs/1206.2944}, abstractNote={Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a “black art” that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.}, note={arXiv:1206.2944 [cs, stat]}, number={arXiv:1206.2944}, publisher={arXiv}, author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.}, year={2012}, month={Aug} }
 
 
 @misc{Surjanovic_Bingham, title={Virtual Library of Simulation Experiments: Test Functions and Datasets}, note={Published: Retrieved September 10, 2022, from http://www.sfu.ca/ ssurjano}, author={Surjanovic, S. and Bingham, D.} }
 
 
 @article{Zitkovic_2010, title={Introduction to stochastic processes-lecture notes}, journal={Department of Mathematics, The University of Texas at Austin}, author={Žitković, Gordan}, year={2010} }
 
 
 @book{billingsley2012probability,
 	title={Probability and Measure},
 	author={Billingsley, P.},
 	isbn={9781118341919},
 	series={Wiley Series in Probability and Statistics},
 	url={https://books.google.ca/books?id=a3gavZbxyJcC},
 	year={2012},
 	publisher={Wiley}
 }
 
@article{hohendorff_introduction_2005,
	title = {An introduction to markov chain monte carlo},
	journal = {University of Toronto, Department of Statistics, supervised reading report (http://www. probability. ca/jeff/grad. html)},
	author = {Hohendorff, Johannes M and Rosenthal, J},
	year = {2005},
	file = {Hohendorff and Rosenthal - 2005 - An introduction to markov chain monte carlo.pdf:/Users/saintcloud/Zotero/storage/CE5HBBBG/Hohendorff and Rosenthal - 2005 - An introduction to markov chain monte carlo.pdf:application/pdf},
}


@book{chung_markov_1960,
	address = {Berlin},
	title = {Markov chains with stationary transition probabilities},
	isbn = {978-3-642-49686-8},
	abstract = {"This book presupposes no knowledge of Markov chains but it does assume the elements of general probability theory as given in a modern introductory course."--Preface},
	language = {eng},
	publisher = {Springer},
	author = {Chung, Kai Lai},
	year = {1960},
	note = {OCLC: 682058891},
	file = {Chung - 1960 - Markov chains with stationary transition probabili.pdf:/Users/saintcloud/Zotero/storage/JGK45P3N/Chung - 1960 - Markov chains with stationary transition probabili.pdf:application/pdf},
}

@inproceedings{ghahramani2011tutorial,
	title={A Tutorial on Gaussian Processes (or why I don’t use SVMs)},
	author={Ghahramani, Zoubin},
	booktitle={Proc. MLSS Workshop Talk Gaussian Processes},
	year={2011}
}

@book{garnett_bayesoptbook_2023,
	author    = {Garnett, Roman},
	title     = {{Bayesian Optimization}},
	year      = {2023},
	publisher = {Cambridge University Press},
	note      = {to appear}
}

@article{wang2020intuitive,
	title={An intuitive tutorial to Gaussian processes regression},
	author={Wang, Jie},
	journal={arXiv preprint arXiv:2009.10862},
	year={2020}
}

@book{rasmussen2006gaussian,
	title={Gaussian processes for machine learning},
	author={Rasmussen, Carl Edward and Williams, Christopher KI and others},
	volume={1},
	year={2006},
	publisher={Springer}
}

@article{wangpoisson,
	title={Poisson Process for Bayesian Optimization},
	author={Wang, Xiaoxing and Li, Jiaxing and Xue, Chao and Liu, Wei and Wang, Chaoyue and Liu, Weifeng and Yang, Xiaokang and Yan, Junchi and Tao, Dacheng}
}

@inproceedings{shah2013bayesian,
	title={Bayesian optimization using Student-t Processes},
	author={Shah, Amar and Wilson, Andrew G and Ghahramani, Zoubin},
	booktitle={NIPS Workshop on Bayesian Optimization},
	year={2013}
}

@article{friedman1991multivariate,
	title={Multivariate adaptive regression splines},
	author={Friedman, Jerome H},
	journal={The annals of statistics},
	volume={19},
	number={1},
	pages={1--67},
	year={1991},
	publisher={Institute of Mathematical Statistics}
}

@book{breiman2017classification,
	title={Classification and regression trees},
	author={Breiman, Leo},
	year={2017},
	publisher={Routledge}
}

@article{denison1998bayesian,
	title={Bayesian mars},
	author={Denison, David GT and Mallick, Bani K and Smith, Adrian FM},
	journal={Statistics and Computing},
	volume={8},
	pages={337--346},
	year={1998},
	publisher={Springer}
}

@article{green1995reversible,
	title={Reversible jump Markov chain Monte Carlo computation and Bayesian model determination},
	author={Green, Peter J},
	journal={Biometrika},
	volume={82},
	number={4},
	pages={711--732},
	year={1995},
	publisher={Oxford University Press}
}

@article{francom2020bass,
	title={Bass: An r package for fitting and performing sensitivity analysis of bayesian adaptive spline surfaces},
	author={Francom, Devin and Sans{\'o}, Bruno},
	journal={Journal of Statistical Software},
	volume={94},
	number={LA-UR-20-23587},
	year={2020},
	publisher={Los Alamos National Lab.(LANL), Los Alamos, NM (United States)}
}

@book{rubinstein2016simulation,
	title={Simulation and the Monte Carlo method},
	author={Rubinstein, Reuven Y and Kroese, Dirk P},
	year={2016},
	publisher={John Wiley \& Sons}
}

@book{robert1999monte,
	title={Monte Carlo statistical methods},
	author={Robert, Christian P and Casella, George and Casella, George},
	volume={2},
	year={1999},
	publisher={Springer}
}

@inproceedings{bergstra2013hyperopt,
	title={Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms},
	author={Bergstra, James and Yamins, Dan and Cox, David D and others},
	booktitle={Proceedings of the 12th Python in science conference},
	volume={13},
	pages={20},
	year={2013},
	organization={Citeseer}
}

@article{balandat2020botorch,
	title={BoTorch: A framework for efficient Monte-Carlo Bayesian optimization},
	author={Balandat, Maximilian and Karrer, Brian and Jiang, Daniel and Daulton, Samuel and Letham, Ben and Wilson, Andrew G and Bakshy, Eytan},
	journal={Advances in neural information processing systems},
	volume={33},
	pages={21524--21538},
	year={2020}
}

@inproceedings{akiba2019optuna,
	title={Optuna: A next-generation hyperparameter optimization framework},
	author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
	pages={2623--2631},
	year={2019}
}

@inproceedings{kaufmann2012thompson,
	title={Thompson sampling: An asymptotically optimal finite-time analysis},
	author={Kaufmann, Emilie and Korda, Nathaniel and Munos, R{\'e}mi},
	booktitle={International conference on algorithmic learning theory},
	pages={199--213},
	year={2012},
	organization={Springer}
}

@inproceedings{kaufmann2012bayesian,
	title={On Bayesian upper confidence bounds for bandit problems},
	author={Kaufmann, Emilie and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
	booktitle={Artificial intelligence and statistics},
	pages={592--600},
	year={2012},
	organization={PMLR}
}

@misc{ enwiki:1176088789,
	author = "{Wikipedia contributors}",
	title = "Kolmogorov extension theorem --- {Wikipedia}{,} The Free Encyclopedia",
	year = "2023",
	url = "https://en.wikipedia.org/w/index.php?title=Kolmogorov_extension_theorem&oldid=1176088789",
	note = "[Online; accessed 25-September-2023]"
}

@misc{ conceptualBO,
	author = {Will Koehrsen},
	title = {A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning},
	url = {\url{https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f}},
	note = {Accessed: 15 Feb 2023},
	year = {2018}
}

@article{wang2023recent,
	title={Recent advances in Bayesian optimization},
	author={Wang, Xilu and Jin, Yaochu and Schmitt, Sebastian and Olhofer, Markus},
	journal={ACM Computing Surveys},
	volume={55},
	number={13s},
	pages={1--36},
	year={2023},
	publisher={ACM New York, NY}
}

@article{bergstra2011algorithms,
	title={Algorithms for hyper-parameter optimization},
	author={Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
	journal={Advances in neural information processing systems},
	volume={24},
	year={2011}
}

@article{grinsztajn2022tree,
	title={Why do tree-based models still outperform deep learning on typical tabular data?},
	author={Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={507--520},
	year={2022}
}

@article{mckay2000comparison,
	title={A comparison of three methods for selecting values of input variables in the analysis of output from a computer code},
	author={McKay, Michael D and Beckman, Richard J and Conover, William J},
	journal={Technometrics},
	volume={42},
	number={1},
	pages={55--61},
	year={2000},
	publisher={Taylor \& Francis}
}

@article{stein1987large,
	title={Large sample properties of simulations using Latin hypercube sampling},
	author={Stein, Michael},
	journal={Technometrics},
	volume={29},
	number={2},
	pages={143--151},
	year={1987},
	publisher={Taylor \& Francis}
}

@article{bergstra2012random,
	title={Random search for hyper-parameter optimization.},
	author={Bergstra, James and Bengio, Yoshua},
	journal={Journal of machine learning research},
	volume={13},
	number={2},
	year={2012}
}

@article{xiao2020efficient,
	title={Efficient hyperparameter optimization in deep learning using a variable length genetic algorithm},
	author={Xiao, Xueli and Yan, Ming and Basodi, Sunitha and Ji, Chunyan and Pan, Yi},
	journal={arXiv preprint arXiv:2006.12703},
	year={2020}
}
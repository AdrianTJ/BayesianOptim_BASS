\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{BYC{\etalchar{+}}13b}

\bibitem[ASY{\etalchar{+}}19]{akiba2019optuna}
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama.
\newblock Optuna: A next-generation hyperparameter optimization framework.
\newblock In {\em Proceedings of the 25th ACM SIGKDD international conference
  on knowledge discovery \& data mining}, pages 2623--2631, 2019.

\bibitem[BB12]{bergstra2012random}
James Bergstra and Yoshua Bengio.
\newblock Random search for hyper-parameter optimization.
\newblock {\em Journal of machine learning research}, 13(2), 2012.

\bibitem[BBBK]{Bergstra_Bardenet_Bengio_Kégl}
James~S Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl.
\newblock Algorithms for hyper-parameter optimization.
\newblock page~9.

\bibitem[BBBK11]{bergstra2011algorithms}
James Bergstra, R{\'e}mi Bardenet, Yoshua Bengio, and Bal{\'a}zs K{\'e}gl.
\newblock Algorithms for hyper-parameter optimization.
\newblock {\em Advances in neural information processing systems}, 24, 2011.

\bibitem[Bil12]{billingsley2012probability}
P.~Billingsley.
\newblock {\em Probability and Measure}.
\newblock Wiley Series in Probability and Statistics. Wiley, 2012.

\bibitem[BKJ{\etalchar{+}}20]{balandat2020botorch}
Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham,
  Andrew~G Wilson, and Eytan Bakshy.
\newblock Botorch: A framework for efficient monte-carlo bayesian optimization.
\newblock {\em Advances in neural information processing systems},
  33:21524--21538, 2020.

\bibitem[Bre17]{breiman2017classification}
Leo Breiman.
\newblock {\em Classification and regression trees}.
\newblock Routledge, 2017.

\bibitem[BYC{\etalchar{+}}13a]{bergstra2013hyperopt}
James Bergstra, Dan Yamins, David~D Cox, et~al.
\newblock Hyperopt: A python library for optimizing the hyperparameters of
  machine learning algorithms.
\newblock In {\em Proceedings of the 12th Python in science conference},
  volume~13, page~20. Citeseer, 2013.

\bibitem[BYC{\etalchar{+}}13b]{Bergstra_Yamins_Cox_others_2013}
James Bergstra, Dan Yamins, David~D Cox, et~al.
\newblock Hyperopt: A python library for optimizing the hyperparameters of
  machine learning algorithms.
\newblock In {\em Proceedings of the 12th Python in science conference},
  volume~13, page~20. Citeseer, 2013.

\bibitem[Chu60]{chung_markov_1960}
Kai~Lai Chung.
\newblock {\em Markov chains with stationary transition probabilities}.
\newblock Springer, Berlin, 1960.
\newblock OCLC: 682058891.

\bibitem[DMS98a]{Denison_Mallick_Smith_1998}
D.~G.~T. Denison, B.~K. Mallick, and A.~F.~M. Smith.
\newblock Bayesian mars.
\newblock {\em Statistics and Computing}, 8(4):337–346, 1998.

\bibitem[DMS98b]{denison1998bayesian}
David~GT Denison, Bani~K Mallick, and Adrian~FM Smith.
\newblock Bayesian mars.
\newblock {\em Statistics and Computing}, 8:337--346, 1998.

\bibitem[DMTK20]{Daxberger_Makarova_Turchetta_Krause_2020}
Erik Daxberger, Anastasia Makarova, Matteo Turchetta, and Andreas Krause.
\newblock Mixed-variable bayesian optimization.
\newblock In {\em Proceedings of the Twenty-Ninth International Joint
  Conference on Artificial Intelligence}, page 2633–2639, Jul 2020.
\newblock arXiv:1907.01329 [cs, stat].

\bibitem[Fra18]{Frazier_2018}
Peter~I. Frazier.
\newblock A tutorial on bayesian optimization.
\newblock (arXiv:1807.02811), Jul 2018.
\newblock arXiv:1807.02811 [cs, math, stat].

\bibitem[Fri91a]{Friedman_1991}
Jerome~H. Friedman.
\newblock Multivariate adaptive regression splines.
\newblock {\em The Annals of Statistics}, 19(1), Mar 1991.

\bibitem[Fri91b]{friedman1991multivariate}
Jerome~H Friedman.
\newblock Multivariate adaptive regression splines.
\newblock {\em The annals of statistics}, 19(1):1--67, 1991.

\bibitem[FS20a]{francom2020bass}
Devin Francom and Bruno Sans{\'o}.
\newblock Bass: An r package for fitting and performing sensitivity analysis of
  bayesian adaptive spline surfaces.
\newblock {\em Journal of Statistical Software}, 94(LA-UR-20-23587), 2020.

\bibitem[FS20b]{Francom_Sansó_2020}
Devin Francom and Bruno Sansó.
\newblock Bass : An r package for fitting and performing sensitivity analysis
  of bayesian adaptive spline surfaces.
\newblock {\em Journal of Statistical Software}, 94(8), 2020.

\bibitem[Gar23]{garnett_bayesoptbook_2023}
Roman Garnett.
\newblock {\em {Bayesian Optimization}}.
\newblock Cambridge University Press, 2023.
\newblock to appear.

\bibitem[Gha11]{ghahramani2011tutorial}
Zoubin Ghahramani.
\newblock A tutorial on gaussian processes (or why i don’t use svms).
\newblock In {\em Proc. MLSS Workshop Talk Gaussian Processes}, 2011.

\bibitem[GMHL20]{Garrido-Merchán_Hernández-Lobato_2020}
Eduardo~C. Garrido-Merchán and Daniel Hernández-Lobato.
\newblock Dealing with categorical and integer-valued variables in bayesian
  optimization with gaussian processes.
\newblock {\em Neurocomputing}, 380:20–35, Mar 2020.
\newblock arXiv:1805.03463 [cs, stat].

\bibitem[GOV22]{grinsztajn2022tree}
L{\'e}o Grinsztajn, Edouard Oyallon, and Ga{\"e}l Varoquaux.
\newblock Why do tree-based models still outperform deep learning on typical
  tabular data?
\newblock {\em Advances in Neural Information Processing Systems}, 35:507--520,
  2022.

\bibitem[Gre95a]{green1995reversible}
Peter~J Green.
\newblock Reversible jump markov chain monte carlo computation and bayesian
  model determination.
\newblock {\em Biometrika}, 82(4):711--732, 1995.

\bibitem[Gre95b]{Green_1995}
Peter~J. Green.
\newblock Reversible jump markov chain monte carlo computation and bayesian
  model determination.
\newblock {\em Biometrika}, 82(4):711–732, 1995.

\bibitem[HR05]{hohendorff_introduction_2005}
Johannes~M Hohendorff and J~Rosenthal.
\newblock An introduction to markov chain monte carlo.
\newblock {\em University of Toronto, Department of Statistics, supervised
  reading report (http://www. probability. ca/jeff/grad. html)}, 2005.

\bibitem[KCG12]{kaufmann2012bayesian}
Emilie Kaufmann, Olivier Capp{\'e}, and Aur{\'e}lien Garivier.
\newblock On bayesian upper confidence bounds for bandit problems.
\newblock In {\em Artificial intelligence and statistics}, pages 592--600.
  PMLR, 2012.

\bibitem[KFB{\etalchar{+}}17]{Klein_Falkner_Bartels_Hennig_Hutter_2017}
Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter.
\newblock Fast bayesian optimization of machine learning hyperparameters on
  large datasets.
\newblock (arXiv:1605.07079), Mar 2017.
\newblock arXiv:1605.07079 [cs, stat].

\bibitem[KKM12]{kaufmann2012thompson}
Emilie Kaufmann, Nathaniel Korda, and R{\'e}mi Munos.
\newblock Thompson sampling: An asymptotically optimal finite-time analysis.
\newblock In {\em International conference on algorithmic learning theory},
  pages 199--213. Springer, 2012.

\bibitem[Koe18]{conceptualBO}
Will Koehrsen.
\newblock A conceptual explanation of bayesian hyperparameter optimization for
  machine learning, 2018.
\newblock Accessed: 15 Feb 2023.

\bibitem[LGN{\etalchar{+}}19]{Luong_Gupta_Nguyen_Rana_Venkatesh_2019}
Phuc Luong, Sunil Gupta, Dang Nguyen, Santu Rana, and Svetha Venkatesh.
\newblock {\em Bayesian Optimization with Discrete Variables}, volume 11919 of
  {\em Lecture Notes in Computer Science}, page 473–484.
\newblock Springer International Publishing, Cham, 2019.

\bibitem[MBC00]{mckay2000comparison}
Michael~D McKay, Richard~J Beckman, and William~J Conover.
\newblock A comparison of three methods for selecting values of input variables
  in the analysis of output from a computer code.
\newblock {\em Technometrics}, 42(1):55--61, 2000.

\bibitem[RCC99]{robert1999monte}
Christian~P Robert, George Casella, and George Casella.
\newblock {\em Monte Carlo statistical methods}, volume~2.
\newblock Springer, 1999.

\bibitem[RK16]{rubinstein2016simulation}
Reuven~Y Rubinstein and Dirk~P Kroese.
\newblock {\em Simulation and the Monte Carlo method}.
\newblock John Wiley \& Sons, 2016.

\bibitem[Ros96]{Ross_1996}
Sheldon~M. Ross.
\newblock {\em Stochastic processes}.
\newblock Wiley series in probability and statistics. Wiley, New York, 2nd ed
  edition, 1996.

\bibitem[RW{\etalchar{+}}06]{rasmussen2006gaussian}
Carl~Edward Rasmussen, Christopher~KI Williams, et~al.
\newblock {\em Gaussian processes for machine learning}, volume~1.
\newblock Springer, 2006.

\bibitem[SBa]{simulationlib}
S.~Surjanovic and D.~Bingham.
\newblock Virtual library of simulation experiments: Test functions and
  datasets.
\newblock Retrieved August 25, 2024, from \url{http://www.sfu.ca/~ssurjano}.

\bibitem[SBb]{Surjanovic_Bingham}
S.~Surjanovic and D.~Bingham.
\newblock Virtual library of simulation experiments: Test functions and
  datasets.
\newblock Published: Retrieved September 10, 2022, from http://www.sfu.ca/
  ssurjano.

\bibitem[SLA12]{Snoek_Larochelle_Adams_2012}
Jasper Snoek, Hugo Larochelle, and Ryan~P. Adams.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock (arXiv:1206.2944), Aug 2012.
\newblock arXiv:1206.2944 [cs, stat].

\bibitem[Ste87]{stein1987large}
Michael Stein.
\newblock Large sample properties of simulations using latin hypercube
  sampling.
\newblock {\em Technometrics}, 29(2):143--151, 1987.

\bibitem[SWG13]{shah2013bayesian}
Amar Shah, Andrew~G Wilson, and Zoubin Ghahramani.
\newblock Bayesian optimization using student-t processes.
\newblock In {\em NIPS Workshop on Bayesian Optimization}, 2013.

\bibitem[Wan20]{wang2020intuitive}
Jie Wang.
\newblock An intuitive tutorial to gaussian processes regression.
\newblock {\em arXiv preprint arXiv:2009.10862}, 2020.

\bibitem[{Wik}23]{enwiki:1176088789}
{Wikipedia contributors}.
\newblock Kolmogorov extension theorem --- {Wikipedia}{,} the free
  encyclopedia, 2023.
\newblock [Online; accessed 25-September-2023].

\bibitem[WJSO23]{wang2023recent}
Xilu Wang, Yaochu Jin, Sebastian Schmitt, and Markus Olhofer.
\newblock Recent advances in bayesian optimization.
\newblock {\em ACM Computing Surveys}, 55(13s):1--36, 2023.

\bibitem[WLX{\etalchar{+}}]{wangpoisson}
Xiaoxing Wang, Jiaxing Li, Chao Xue, Wei Liu, Chaoyue Wang, Weifeng Liu,
  Xiaokang Yang, Junchi Yan, and Dacheng Tao.
\newblock Poisson process for bayesian optimization.

\bibitem[XYB{\etalchar{+}}20]{xiao2020efficient}
Xueli Xiao, Ming Yan, Sunitha Basodi, Chunyan Ji, and Yi~Pan.
\newblock Efficient hyperparameter optimization in deep learning using a
  variable length genetic algorithm.
\newblock {\em arXiv preprint arXiv:2006.12703}, 2020.

\bibitem[Ži10]{Zitkovic_2010}
Gordan Žitković.
\newblock Introduction to stochastic processes-lecture notes.
\newblock {\em Department of Mathematics, The University of Texas at Austin},
  2010.

\end{thebibliography}
